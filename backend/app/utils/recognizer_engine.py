from typing import List, Dict, Any, Optional
# ì—¬ëŸ¬ë¶„ì˜ AnalyzerEngine ì½”ë“œê°€ ì˜ì¡´í•˜ëŠ” í´ë˜ìŠ¤ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
# ì´ íŒŒì¼ë“¤ì´ app/utils í´ë”ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from app.utils.recognizer_registry import RecognizerRegistry
from app.utils.ner.NER_engine import NerEngine
from app.utils.entity import Entity, EntityGroup

# ì—¬ëŸ¬ë¶„ì´ ì œê³µí•œ AnalyzerEngine í´ë˜ìŠ¤
class AnalyzerEngine:
    """
    ê·œì¹™ ê¸°ë°˜(RecognizerRegistry) + NER ê¸°ë°˜(NerEngine) ê²°ê³¼ë¥¼
    í•˜ë‚˜ì˜ EntityGroupìœ¼ë¡œ í†µí•©í•˜ì—¬ ë°˜í™˜.
    """
    def __init__(self, db_client=None):
        print("...AnalyzerEngine ì´ˆê¸°í™” ì¤‘...")
        self.db_client = db_client
        self.registry = RecognizerRegistry(db_client=db_client)
        self.registry.load_predefined_recognizers()

        self.nlp_engine = NerEngine()
        print("~AnalyzerEngine ì¤€ë¹„ ì™„ë£Œ~")

    async def load_custom_entities(self):
        """MongoDBì—ì„œ ì»¤ìŠ¤í…€ ì—”í‹°í‹° ë¡œë“œ"""
        if self.db_client is not None:
            print("ğŸ“‹ ì»¤ìŠ¤í…€ ì—”í‹°í‹° ë¡œë“œ ì¤‘...")
            await self.registry.load_custom_recognizers()

    def analyze(self, text: str) -> EntityGroup:
        regex_group = self.registry.regex_analyze(text)
        ner_group = self.nlp_engine.ner_analyze(text)
        combined = self._merge_groups(regex_group, ner_group)
        combined.entities = self._dedup_and_sort(combined.entities)
        
        print("\n\nìµœì¢… ë¶„ì„ ê²°ê³¼(EntityGroup):")
        for e in combined.entities:
            print(f" - {e.entity}: '{e.word}' ({e.start}, {e.end}) score={e.score:.2f}")

        return combined

    @staticmethod
    def _merge_groups(a: EntityGroup, b: EntityGroup) -> EntityGroup:
        merged = EntityGroup(list(a.entities))
        for e in b.entities:
            replaced = False
            for i, ea in enumerate(merged.entities):
                if ea.entity == e.entity and (ea.start < e.end and e.start < ea.end):
                    len_a = ea.end - ea.start
                    len_b = e.end - e.start
                    if len_b > len_a or (len_b == len_a and e.score > ea.score):
                        merged.entities[i] = e
                    replaced = True
                    break
            if not replaced:
                merged.entities.append(e)
        return merged

    @staticmethod
    def _dedup_and_sort(entities: List[Entity]) -> List[Entity]:
        seen = set()
        uniq: List[Entity] = []
        for e in entities:
            key = (e.entity, e.start, e.end, e.word)
            if key in seen:
                continue
            seen.add(key)
            uniq.append(e)
        uniq.sort(key=lambda x: (x.start, x.end))
        return uniq


def find_text_coordinates_in_ocr(text: str, start_pos: int, end_pos: int, ocr_pages: List[Dict]):
    """
    PII í…ìŠ¤íŠ¸ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ë‚´ ìœ„ì¹˜ë¥¼ OCR í•„ë“œì˜ boundingPoly ì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ë™ì¼í•œ í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¤ëŠ” ê²½ìš°, start_posì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•„ë“œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    coordinates = []
    current_pos = 0
    target_text = text[start_pos:end_pos].strip()
    
    print(f"[ì¢Œí‘œ ê²€ìƒ‰] íƒ€ê²Ÿ í…ìŠ¤íŠ¸: '{target_text}' (ìœ„ì¹˜: {start_pos}-{end_pos})")
    
    for page in ocr_pages:
        page_index = page.get("pageIndex", 0)
        fields = page.get("fields", [])
        
        print(f"[ì¢Œí‘œ ê²€ìƒ‰] í˜ì´ì§€ {page_index} - í•„ë“œ ìˆ˜: {len(fields)}")
        
        for field_idx, field in enumerate(fields):
            field_text = field.get("text", "").strip()
            if not field_text:
                current_pos += 1
                continue
                
            field_start = current_pos
            field_end = current_pos + len(field_text)
            
            # **í•µì‹¬ ìˆ˜ì •: start_posê°€ í•„ë“œ ë²”ìœ„ ë‚´ì— ì •í™•íˆ í¬í•¨ë˜ëŠ” ê²½ìš°ë§Œ ë§¤ì¹­**
            is_exact_match = False
            
            # 1. start_posê°€ ì´ í•„ë“œì˜ ì‹œì‘ ìœ„ì¹˜ì™€ ì¼ì¹˜í•˜ê³ , í…ìŠ¤íŠ¸ë„ ì¼ì¹˜
            if field_start == start_pos and target_text == field_text:
                is_exact_match = True
                print(f"[ì¢Œí‘œ ê²€ìƒ‰] ì •í™•í•œ ìœ„ì¹˜ ë§¤ì¹­: í•„ë“œ ì‹œì‘({field_start}) == PII ì‹œì‘({start_pos})")
            
            # 2. start_posê°€ í•„ë“œ ë‚´ë¶€ì— ìˆê³ , í•„ë“œê°€ íƒ€ê²Ÿ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨
            elif start_pos >= field_start and start_pos < field_end and target_text in field_text:
                # í•„ë“œ ë‚´ì—ì„œ íƒ€ê²Ÿ í…ìŠ¤íŠ¸ì˜ ìƒëŒ€ ìœ„ì¹˜ í™•ì¸
                relative_pos = start_pos - field_start
                if field_text[relative_pos:relative_pos + len(target_text)] == target_text:
                    is_exact_match = True
                    print(f"[ì¢Œí‘œ ê²€ìƒ‰] í•„ë“œ ë‚´ë¶€ ë§¤ì¹­: PII({start_pos}) in Field({field_start}-{field_end})")
            
            if is_exact_match:
                bounding_poly = field.get("boundingPoly", {})
                vertices = bounding_poly.get("vertices", [])
                
                if len(vertices) >= 4:
                    x_coords = [v.get("x", 0) for v in vertices]
                    y_coords = [v.get("y", 0) for v in vertices]
                    
                    x1, x2 = min(x_coords), max(x_coords)
                    y1, y2 = min(y_coords), max(y_coords)
                    
                    margin = 2
                    bbox = [
                        max(0, x1 - margin), 
                        max(0, y1 - margin), 
                        x2 + margin, 
                        y2 + margin
                    ]
                    
                    coordinates.append({
                        "pageIndex": page_index,
                        "bbox": bbox,
                        "field_text": field_text,
                        "vertices": vertices
                    })
                    
                    print(f"[ì¢Œí‘œ ë§¤ì¹­ ì„±ê³µ] PII '{target_text}' -> í˜ì´ì§€ {page_index}, bbox: {bbox}")
                    
                    # **ì¤‘ìš”: ì •í™•í•œ ë§¤ì¹­ì„ ì°¾ì•˜ìœ¼ë¯€ë¡œ ì¦‰ì‹œ ë°˜í™˜ (ì¤‘ë³µ ë°©ì§€)**
                    return coordinates
            
            current_pos = field_end + 1
    
    if not coordinates:
        print(f"[ì¢Œí‘œ ë§¤ì¹­ ì‹¤íŒ¨] PII '{target_text}'ì— ëŒ€í•œ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    return coordinates


async def recognize_pii_in_text(text_content: str, ocr_data: Optional[Dict] = None, db_client=None):
    """
    í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ìµœì¢… í•¨ìˆ˜
    OCR ë°ì´í„°ê°€ ìˆìœ¼ë©´ PIIì˜ ì¢Œí‘œ ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜
    db_clientë¥¼ ì „ë‹¬í•˜ë©´ MongoDBì˜ ì»¤ìŠ¤í…€ ì—”í‹°í‹°ë„ ì‚¬ìš©
    """
    analyzer = AnalyzerEngine(db_client=db_client)

    # ì»¤ìŠ¤í…€ ì—”í‹°í‹° ë¡œë“œ (db_clientê°€ ìˆëŠ” ê²½ìš°)
    if db_client is not None:
        await analyzer.load_custom_entities()

    result = analyzer.analyze(text_content)

    # FastAPIê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
    pii_entities_list = []
    for e in result.entities:
        entity_dict = {
            "text": e.word,
            "type": e.entity,
            "score": e.score,
            "start_char": e.start,
            "end_char": e.end,
        }

        # OCR ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¢Œí‘œ ì •ë³´ë„ ì¶”ê°€
        if ocr_data and "pages" in ocr_data:
            coordinates = find_text_coordinates_in_ocr(
                text_content, e.start, e.end, ocr_data["pages"]
            )
            entity_dict["coordinates"] = coordinates

        pii_entities_list.append(entity_dict)

    return {
        "full_text": text_content,
        "pii_entities": pii_entities_list
    }