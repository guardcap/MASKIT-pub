# ğŸ“ RAG í…ŒìŠ¤íŠ¸ ì½”ë“œ
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
import pickle

# âœ” ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(
    model_name="upskyy/e5-base-korean",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

vectorstore = FAISS.load_local("faiss_policy_db", embedding_model, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# âœ” LLM ë¡œë“œ (Mistral via Ollama)
llm = Ollama(model="mistral")

# âœ” RAG ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# âœ” ì‚¬ìš©ì ì§ˆë¬¸
query = input("â“ ì§ˆë¬¸ ì…ë ¥: ")
result = qa_chain.invoke({"query": query})

# âœ” ë‹µë³€ ì¶œë ¥
print("\nğŸ“ [LLM ë‹µë³€]\n" + result["result"])

# âœ” ì°¸ì¡°ëœ ì²­í¬ ì¶œë ¥
source_docs = result["source_documents"]
print("\nğŸ“„ [ì°¸ì¡°ëœ ì²­í¬]")
for doc in source_docs:
    meta = doc.metadata
    preview = doc.page_content[:150].replace("\n", " ")
    print(f"- ì²­í¬ID: {meta.get('chunk_id')} / ì¥: {meta.get('chapter')} / ì¡°ë¬¸: {meta.get('section')}\n  ë‚´ìš© ì¼ë¶€: {preview}...")
