# ğŸ“ chunk_and_save_all.py
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import pickle
import re

# âœ” ë¬¸ì„œ ë¡œë“œ
FILE_PATH = "policy.md"
loader = TextLoader(FILE_PATH, encoding="utf-8")
docs = loader.load()
print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {FILE_PATH} ({len(docs[0].page_content)}ì)")

# âœ” ì»¤ìŠ¤í…€ ì²­í¬ í•¨ìˆ˜
def custom_chunk_policy_md(text):
    chunks = []
    current_chapter = ""
    current_chunk = ""
    lines = text.splitlines()

    for line in lines:
        if line.startswith("## ì œ"):
            if current_chunk:
                chunks.append((current_chapter, current_chunk.strip()))
                current_chunk = ""
            current_chapter = line.strip()
            current_chunk += line + "\n"
        elif line.startswith("### ì œ"):
            if current_chunk:
                chunks.append((current_chapter, current_chunk.strip()))
                current_chunk = ""
            current_chunk += line + "\n"
        else:
            current_chunk += line + "\n"

    if current_chunk:
        chunks.append((current_chapter, current_chunk.strip()))
    return chunks  # [(chapter_title, chunk_content)]

# âœ” ì²­í¬ ìƒì„± ë° íƒœê¹…
raw_text = docs[0].page_content
chunk_data = custom_chunk_policy_md(raw_text)
chunk_docs = []

for i, (chapter, content) in enumerate(chunk_data):
    metadata = {
        "source": FILE_PATH,
        "chunk_id": i + 1,
        "chapter": chapter
    }
    match = re.search(r"### (ì œ\d+ì¡°)", content)
    metadata["section"] = match.group(1) if match else "ê¸°íƒ€"

    doc = Document(page_content=content, metadata=metadata)
    chunk_docs.append(doc)

print(f"âœ… ì»¤ìŠ¤í…€ ì²­í¬ ì™„ë£Œ: {len(chunk_docs)}ê°œ")

# âœ” ì²­í¬ í…ìŠ¤íŠ¸ ì €ì¥
OUTPUT_PATH = "chunk_output.txt"
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    for doc in chunk_docs:
        meta = doc.metadata
        f.write(f"\n--- ì²­í¬ {meta['chunk_id']} ---\n")
        f.write(f"[ì¥: {meta['chapter']}]\n")
        f.write(f"[ì¡°ë¬¸: {meta['section']}]\n")
        f.write(f"[ì†ŒìŠ¤: {meta['source']}]\n")
        f.write(doc.page_content.strip())
        f.write("\n")
print(f"ğŸ—ƒ ì²­í¬ + íƒœê·¸ ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")

# âœ” ì²­í¬ ê°ì²´ ì €ì¥
with open("chunks_tagged.pkl", "wb") as f:
    pickle.dump(chunk_docs, f)
print("ğŸ“† ì²­í¬ ê°ì²´ ì €ì¥ ì™„ë£Œ: chunks_tagged.pkl")

# âœ” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = HuggingFaceEmbeddings(
    model_name="upskyy/e5-base-korean",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# âœ” FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥
vectorstore = FAISS.from_documents(chunk_docs, embedding_model)
vectorstore.save_local("faiss_policy_db")
print("ğŸ“† ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ: faiss_policy_db")
