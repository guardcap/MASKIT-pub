---
language: 
  - multilingual
  - af
  - am
  - ar
  - as
  - az
  - be
  - bg
  - bn
  - br
  - bs
  - ca
  - cs
  - cy
  - da
  - de
  - el
  - en
  - eo
  - es
  - et
  - eu
  - fa
  - fi
  - fr
  - fy
  - ga
  - gd
  - gl
  - gu
  - ha
  - he
  - hi
  - hr
  - hu
  - hy
  - id
  - is
  - it
  - ja
  - jv
  - ka
  - kk
  - km
  - kn
  - ko
  - ku
  - ky
  - la
  - lo
  - lt
  - lv
  - mg
  - mk
  - ml
  - mn
  - mr
  - ms
  - my
  - ne
  - nl
  - 'no'
  - om
  - or
  - pa
  - pl
  - ps
  - pt
  - ro
  - ru
  - sa
  - sd
  - si
  - sk
  - sl
  - so
  - sq
  - sr
  - su
  - sv
  - sw
  - ta
  - te
  - th
  - tl
  - tr
  - ug
  - uk
  - ur
  - uz
  - vi
  - xh
  - yi
  - zh
license: mit
library_name: sentence-transformers
tags:
  - korean
  - sentence-transformers
  - transformers
  - multilingual
  - sentence-transformers
  - sentence-similarity
  - feature-extraction
base_model: intfloat/multilingual-e5-base
datasets: []
metrics:
- pearson_cosine
- spearman_cosine
- pearson_manhattan
- spearman_manhattan
- pearson_euclidean
- spearman_euclidean
- pearson_dot
- spearman_dot
- pearson_max
- spearman_max
widget:
- source_sentence: ì´ì§‘íŠ¸ êµ°ëŒ€ê°€ í˜•ì œì• ë¥¼ ë‹¨ì†í•˜ë‹¤
  sentences:
  - ì´ì§‘íŠ¸ì˜ êµ°ëŒ€ê°€ ë¬´ìŠ¬ë¦¼ í˜•ì œì• ë¥¼ ë‹¨ì†í•˜ë‹¤
  - ì•„ë¥´í—¨í‹°ë‚˜ì˜ ê¸°ì˜ˆë¥´ëª¨ ì½”ë¦¬ì•„ì™€ ë„¤ëœë€ë“œì˜ ë§ˆí‹´ ë²„ì»¤í¬ì˜ ë˜ ë‹¤ë¥¸ ì¤€ê²°ìŠ¹ì „ë„ ë§¤ë ¥ì ì´ë‹¤.
  - ê·¸ê²ƒì´ ì‚¬ì‹¤ì¼ ìˆ˜ë„ ìˆë‹¤ê³  ìƒê°í•˜ëŠ” ê²ƒì€ ì¬ë¯¸ìˆë‹¤.
- source_sentence: ì˜¤, ê·¸ë¦¬ê³  ë‹¤ì‹œ ê²°í˜¼ì€ ê·¼ë³¸ì ì¸ ì¸ê¶Œì´ë¼ê³  ì£¼ì¥í•œë‹¤.
  sentences:
  - íŠ¹íˆ ê²°í˜¼ì€ ê·¼ë³¸ì ì¸ ì¸ê¶Œì´ë¼ê³  ë§í•œ í›„ì—.
  - í•´ë³€ì— ìˆëŠ” í‘ì¸ê³¼ ê·¸ì˜ ê°œ...
  - ì´ë€ì€ í•µ í”„ë¡œê·¸ë¨ì´ í‰í™”ì ì¸ ëª©ì ì„ ìœ„í•œ ê²ƒì´ë¼ê³  ì£¼ì¥í•œë‹¤
- source_sentence: ë‹´ë°° í”¼ìš°ëŠ” ì—¬ì.
  sentences:
  - ì´ê²ƒì€ ë‚´ê°€ ì˜êµ­ì˜ ì•„ì„œ ì•ˆë°ë¥´ì„¼ ì‚¬ì—…ë¶€ì˜ íŒŒíŠ¸ë„ˆì¸ ì§ ì™€ë””ì•„ë¥¼ ì•„ì„œ ì•ˆë°ë¥´ì„¼ ê²½ì˜ì§„ì´ ì„ íƒí•œ ê²ƒë³´ë‹¤ ë˜ë¦¬ ì›¨ì¸ë°”íë¥¼ ì•ˆë°ë¥´ì„¼ ì›”ë“œì™€ì´ë“œì˜
    ê²½ì˜ íŒŒíŠ¸ë„ˆë¡œ ìŠ¹ê³„í•˜ê¸° ìœ„í•´ ì•ˆë°ë¥´ì„¼ ì»¨ì„¤íŒ… ì‚¬ì—…ë¶€(í˜„ì¬ì˜ ì—‘ì„¼ì¸„ì–´ë¼ê³  ì•Œë ¤ì ¸ ìˆìŒ)ì˜ ì „ ê´€ë¦¬ íŒŒíŠ¸ë„ˆì¸ ì¡°ì§€ ìƒ¤íŒì— ëŒ€í•œ ì§€ì§€ë¥¼ í‘œëª…í–ˆì„
    ë•Œ ê°€ì¥ ëª…ë°±í–ˆë‹¤.
  - í•œ ì—¬ìê°€ ë¬¼ í•œ ì”ì„ ë§ˆì‹œê³  ìˆë‹¤.
  - í•œ ì—¬ì„±ì´ ë‹´ë°°ë¥¼ í”¼ìš°ë©´ì„œ ì²­êµ¬ì„œë¥¼ ì§€ë¶ˆí•˜ëŠ” ê²ƒì„ ì••ë„í–ˆë‹¤.
- source_sentence: ë£¨ì´ 15ì„¸ì˜ ì†Œìˆ˜ ë¯¼ì¡±ì¸ í”„ë‘ìŠ¤ì˜ ë¦¬ì  íŠ¸ì¸ í•„ë¦¬í”„ ë„ë¥¼ë ˆì•™ ì‹œëŒ€ì—ëŠ” ì•…ëª… ë†’ì€ ì˜¤ë¥´ê°€ì¦˜ì˜ í˜„ì¥ì´ì—ˆë‹¤.
  sentences:
  - í•„ë¦½ ë„ë¦°ìŠ¤ëŠ” ë£¨ì´ 15ì„¸ê°€ 70ëŒ€ì˜€ì„ ë•Œ ì„­ì •ì´ì—ˆë‹¤.
  - í–‰ë³µí•œ ì–´ë¦° ì†Œë…„ì´ ì»¤ë‹¤ë€ ì—˜ëª¨ ì¸í˜•ì´ ìˆëŠ” ì˜ìì— ì•‰ì•„ ìˆë‹¤.
  - í•„ë¦¬í”„ ë„ë¥¼ë ˆì•™ ì‹œëŒ€ì—ëŠ” ê·¸ê³³ì—ì„œ ë§ì€ ìœ ëª…í•œ ì˜¤ë¥´ê°€ì¦˜ì´ ì¼ì–´ë‚¬ë‹¤.
- source_sentence: ë‘ ë‚¨ìê°€ ì•ˆì—ì„œ ì¼í•˜ê³  ìˆë‹¤
  sentences:
  - êµ­ë¦½ê³µì›ì—ì„œ ê°€ì¥ í° ë§ˆì„ì¸ ì¼€ìŠ¤ìœ…ì˜ ì¸êµ¬ëŠ” ë§¤ë…„ ì—¬ë¦„ ë“±ì‚°ê°, ë±ƒì‚¬ëŒ, ê´€ê´‘ê°ì´ ë„ì°©í•¨ì— ë”°ë¼ ì¦ê°€í•œë‹¤.
  - ë‘ ë‚¨ìê°€ ì¶•êµ¬ ê²½ê¸°ë¥¼ ë³´ê³  ê°„ì‹ì„ ë¨¹ëŠ”ë‹¤.
  - ë‘ ë‚¨ìê°€ ì§‘ì— íƒ€ì¼ì„ ê¹”ì•˜ë‹¤.
pipeline_tag: sentence-similarity
model-index:
- name: upskyy/e5-base-korean
  results:
  - task:
      type: semantic-similarity
      name: Semantic Similarity
    dataset:
      name: sts dev
      type: sts-dev
    metrics:
    - type: pearson_cosine
      value: 0.8593935914692068
      name: Pearson Cosine
    - type: spearman_cosine
      value: 0.8572594228080116
      name: Spearman Cosine
    - type: pearson_manhattan
      value: 0.8217336375412545
      name: Pearson Manhattan
    - type: spearman_manhattan
      value: 0.8280050978871264
      name: Spearman Manhattan
    - type: pearson_euclidean
      value: 0.8208931119126335
      name: Pearson Euclidean
    - type: spearman_euclidean
      value: 0.8277058727421436
      name: Spearman Euclidean
    - type: pearson_dot
      value: 0.8187961699085111
      name: Pearson Dot
    - type: spearman_dot
      value: 0.8236175658758088
      name: Spearman Dot
    - type: pearson_max
      value: 0.8593935914692068
      name: Pearson Max
    - type: spearman_max
      value: 0.8572594228080116
      name: Spearman Max
---

# upskyy/e5-base-korean

This model is korsts and kornli finetuning model from [intfloat/multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base). It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.

## Model Details

### Model Description
- **Model Type:** Sentence Transformer
- **Base model:** [intfloat/multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) <!-- at revision d13f1b27baf31030b7fd040960d60d909913633f -->
- **Maximum Sequence Length:** 512 tokens
- **Output Dimensionality:** 768 tokens
- **Similarity Function:** Cosine Similarity
<!-- - **Training Dataset:** Unknown -->
<!-- - **Language:** Unknown -->
<!-- - **License:** Unknown -->


### Full Model Architecture

```
SentenceTransformer(
  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: XLMRobertaModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})
)
```


## Usage

### Usage (Sentence-Transformers)


First install the Sentence Transformers library:

```bash
pip install -U sentence-transformers
```

Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer

# Download from the ğŸ¤— Hub
model = SentenceTransformer("upskyy/e5-base-korean")

# Run inference
sentences = [
    'ì•„ì´ë¥¼ ê°€ì§„ ì—„ë§ˆê°€ í•´ë³€ì„ ê±·ëŠ”ë‹¤.',
    'ë‘ ì‚¬ëŒì´ í•´ë³€ì„ ê±·ëŠ”ë‹¤.',
    'í•œ ë‚¨ìê°€ í•´ë³€ì—ì„œ ê°œë¥¼ ì‚°ì±…ì‹œí‚¨ë‹¤.',
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [3, 768]

# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [3, 3]
```

### Usage (HuggingFace Transformers)

Without sentence-transformers, you can use the model like this: 
First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

```python
from transformers import AutoTokenizer, AutoModel
import torch


# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Sentences we want sentence embeddings for
sentences = ["ì•ˆë…•í•˜ì„¸ìš”?", "í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ì„ ìœ„í•œ ë²„íŠ¸ ëª¨ë¸ì…ë‹ˆë‹¤."]

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained("upskyy/e5-base-korean")
model = AutoModel.from_pretrained("upskyy/e5-base-korean")

# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

# Perform pooling. In this case, mean pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input["attention_mask"])

print("Sentence embeddings:")
print(sentence_embeddings)
```


## Evaluation

### Metrics

#### Semantic Similarity
* Dataset: `sts-dev`
* Evaluated with [<code>EmbeddingSimilarityEvaluator</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.EmbeddingSimilarityEvaluator)

| Metric             | Value      |
| :----------------- | :--------- |
| pearson_cosine     | 0.8594     |
| spearman_cosine    | 0.8573     |
| pearson_manhattan  | 0.8217     |
| spearman_manhattan | 0.828      |
| pearson_euclidean  | 0.8209     |
| spearman_euclidean | 0.8277     |
| pearson_dot        | 0.8188     |
| spearman_dot       | 0.8236     |
| **pearson_max**    | **0.8594** |
| **spearman_max**   | **0.8573** |

<!--
## Bias, Risks and Limitations

*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->

<!--
### Recommendations

*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->

### Framework Versions
- Python: 3.10.13
- Sentence Transformers: 3.0.1
- Transformers: 4.42.4
- PyTorch: 2.3.0+cu121
- Accelerate: 0.30.1
- Datasets: 2.16.1
- Tokenizers: 0.19.1

## Citation

### BibTeX


```bibtex
@article{wang2024multilingual,
  title={Multilingual E5 Text Embeddings: A Technical Report},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2402.05672},
  year={2024}
}
```

```bibtex
@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/1908.10084",
}
```
