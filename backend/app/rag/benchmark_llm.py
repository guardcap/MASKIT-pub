"""
LLM ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
Ollama llama3 vs OpenAI gpt-4o ë¹„êµ

ì‹¤í–‰ ë°©ë²•:
    # OpenAI ì‚¬ìš© ì‹œ API í‚¤ ì„¤ì • í•„ìš”
    export OPENAI_API_KEY=sk-proj-...
    python benchmark_llm.py
"""
import os
import sys
import time
from typing import Dict, List, Tuple
from statistics import mean, median, stdev

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from agent.llm_factory import get_llm
from langchain_core.messages import HumanMessage, SystemMessage


# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
TEST_CASES = [
    {
        "name": "ê°„ë‹¨í•œ ë¶„ë¥˜ ì‘ì—…",
        "system": "ë‹¹ì‹ ì€ ì´ë©”ì¼ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "prompt": """ë‹¤ìŒ ì´ë©”ì¼ì˜ ë°œì‹ ì ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”:
ì´ë©”ì¼: "ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì² ìˆ˜ ê³ ê°ë‹˜. ê²¬ì ì„œë¥¼ ë³´ë‚´ë“œë¦½ë‹ˆë‹¤."

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€: internal, external_customer, external_partner, unknown
ë‹µë³€ë§Œ ì¶œë ¥í•˜ì„¸ìš”.""",
        "expected_tokens": 10
    },
    {
        "name": "ì¤‘ê°„ ë³µì¡ë„ ì¶”ë¡ ",
        "system": "ë‹¹ì‹ ì€ ê°œì¸ì •ë³´ ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "prompt": """ë‹¤ìŒ ìƒí™©ì—ì„œ ì „í™”ë²ˆí˜¸ë¥¼ ë§ˆìŠ¤í‚¹í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”:

ìƒí™©: ì™¸ë¶€ ê³ ê°ì´ ë¨¼ì € ë¬¸ì˜ ì´ë©”ì¼ì„ ë³´ëƒˆê³ , íšŒì‚¬ ë‹´ë‹¹ìê°€ íšŒì‹ í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.
ì „í™”ë²ˆí˜¸: ë‹´ë‹¹ìì˜ íšŒì‚¬ ì „í™”ë²ˆí˜¸ (02-1234-5678)

ë§ˆìŠ¤í‚¹ í•„ìš” ì—¬ë¶€ì™€ ì´ìœ ë¥¼ 50ì ì´ë‚´ë¡œ ì„¤ëª…í•˜ì„¸ìš”.""",
        "expected_tokens": 50
    },
    {
        "name": "ë³µì¡í•œ ë²•ë¥  í•´ì„",
        "system": "ë‹¹ì‹ ì€ í•œêµ­ ê°œì¸ì •ë³´ë³´í˜¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "prompt": """ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œ15ì¡° 1í•­ 1í˜¸ì— ë”°ë¥´ë©´:
"ì •ë³´ì£¼ì²´ì˜ ë™ì˜ë¥¼ ë°›ì€ ê²½ìš° ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆë‹¤"

ì§ˆë¬¸: ê³ ê°ì´ ë¨¼ì € ê²¬ì  ìš”ì²­ ì´ë©”ì¼ì„ ë³´ë‚¸ ê²½ìš°, ì´ê²ƒì´ 'ëª…ì‹œì  ë™ì˜'ë¡œ ê°„ì£¼ë  ìˆ˜ ìˆë‚˜ìš”?
ë²•ì  ê·¼ê±°ì™€ í•¨ê»˜ 150ì ì´ë‚´ë¡œ ë‹µë³€í•˜ì„¸ìš”.""",
        "expected_tokens": 150
    },
    {
        "name": "ê¸´ í…ìŠ¤íŠ¸ ìƒì„±",
        "system": "ë‹¹ì‹ ì€ ì´ë©”ì¼ ë§ˆìŠ¤í‚¹ ì •ì±… ì‘ì„±ìì…ë‹ˆë‹¤.",
        "prompt": """ë‹¤ìŒ PII ë§ˆìŠ¤í‚¹ ê²°ì •ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”:

PII: ì´ë¦„ "ê¹€ì² ìˆ˜"
ë§¥ë½: ì™¸ë¶€ í˜‘ë ¥ì‚¬ì—ê²Œ ë³´ë‚´ëŠ” í”„ë¡œì íŠ¸ ë‹´ë‹¹ì ì •ë³´ ê³µìœ  ì´ë©”ì¼
ê²°ì •: ë§ˆìŠ¤í‚¹í•˜ì§€ ì•ŠìŒ

ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì—¬ 300ì ì´ë‚´ë¡œ ì„¤ëª…:
1. ë²•ì  ê·¼ê±°
2. ë¹„ì¦ˆë‹ˆìŠ¤ ì •ë‹¹ì„±
3. ë¦¬ìŠ¤í¬ í‰ê°€
4. ê¶Œê³ ì‚¬í•­""",
        "expected_tokens": 300
    },
    {
        "name": "êµ¬ì¡°í™”ëœ ì¶œë ¥",
        "system": "ë‹¹ì‹ ì€ ë°ì´í„° êµ¬ì¡° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "prompt": """ë‹¤ìŒ ì´ë©”ì¼ì—ì„œ PIIë¥¼ ì¶”ì¶œí•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:

ì´ë©”ì¼: "ì•ˆë…•í•˜ì„¸ìš”, í™ê¸¸ë™ ê³ ê°ë‹˜. ì—°ë½ì²˜ 010-1234-5678ë¡œ íšŒì‹ ë“œë¦½ë‹ˆë‹¤."

ì¶œë ¥ í˜•ì‹:
{
  "entities": [
    {"type": "NAME", "value": "...", "confidence": 0.0-1.0},
    {"type": "PHONE", "value": "...", "confidence": 0.0-1.0}
  ]
}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.""",
        "expected_tokens": 100
    }
]


class LLMBenchmark:
    """LLM ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.results = {}

    def test_llm_latency(
        self,
        model_name: str,
        system_prompt: str,
        user_prompt: str,
        num_runs: int = 3
    ) -> Dict:
        """
        ë‹¨ì¼ LLM í…ŒìŠ¤íŠ¸ ì‹¤í–‰

        Args:
            model_name: ëª¨ë¸ëª… (ì˜ˆ: "llama3", "gpt-4o")
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            num_runs: ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜

        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        latencies = []
        responses = []
        errors = 0

        print(f"\n   Testing {model_name}...", end=" ", flush=True)

        for run in range(num_runs):
            try:
                # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                llm = get_llm(model=model_name, temperature=0.0)

                # ë©”ì‹œì§€ êµ¬ì„±
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]

                # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                response = llm.invoke(messages)
                end_time = time.time()

                latency = end_time - start_time
                latencies.append(latency)
                responses.append(response.content)

                print(".", end="", flush=True)

            except Exception as e:
                print("X", end="", flush=True)
                errors += 1
                continue

        print(f" Done")

        if not latencies:
            return {
                "success": False,
                "error": "All runs failed",
                "errors": errors
            }

        return {
            "success": True,
            "model": model_name,
            "runs": num_runs,
            "errors": errors,
            "latencies": latencies,
            "mean_latency": mean(latencies),
            "median_latency": median(latencies),
            "min_latency": min(latencies),
            "max_latency": max(latencies),
            "stdev_latency": stdev(latencies) if len(latencies) > 1 else 0.0,
            "total_time": sum(latencies),
            "sample_response": responses[0][:100] if responses else None
        }

    def run_benchmark(
        self,
        models: List[str],
        num_runs: int = 3
    ) -> Dict:
        """
        ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

        Args:
            models: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
            num_runs: ê° í…ŒìŠ¤íŠ¸ë‹¹ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜

        Returns:
            ì „ì²´ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "=" * 80)
        print(" LLM ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ".center(80, "="))
        print("=" * 80)
        print(f"\nğŸ“Š ì„¤ì •:")
        print(f"   - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(TEST_CASES)}ê°œ")
        print(f"   - ëª¨ë¸: {', '.join(models)}")
        print(f"   - ê° í…ŒìŠ¤íŠ¸ë‹¹ ì‹¤í–‰ íšŸìˆ˜: {num_runs}íšŒ")
        print(f"   - ì´ API í˜¸ì¶œ: {len(TEST_CASES) * len(models) * num_runs}íšŒ\n")

        results = {}

        for test_case in TEST_CASES:
            test_name = test_case["name"]
            print(f"\n{'â”€' * 80}")
            print(f"ğŸ“ í…ŒìŠ¤íŠ¸: {test_name}")
            print(f"   ì˜ˆìƒ í† í°: ~{test_case['expected_tokens']} tokens")

            test_results = {}

            for model in models:
                result = self.test_llm_latency(
                    model_name=model,
                    system_prompt=test_case["system"],
                    user_prompt=test_case["prompt"],
                    num_runs=num_runs
                )
                test_results[model] = result

            results[test_name] = test_results

        self.results = results
        return results

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n\n" + "=" * 80)
        print(" ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ".center(80, "="))
        print("=" * 80)

        if not self.results:
            print("\nâŒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ëª¨ë¸ë³„ ì „ì²´ í‰ê·  ê³„ì‚°
        all_models = set()
        for test_results in self.results.values():
            all_models.update(test_results.keys())

        model_stats = {model: [] for model in all_models}

        for test_name, test_results in self.results.items():
            print(f"\n{'â”€' * 80}")
            print(f"ğŸ“ {test_name}")
            print(f"{'â”€' * 80}")

            for model, result in test_results.items():
                if not result.get("success"):
                    print(f"\nâŒ {model}: ì‹¤íŒ¨ ({result.get('error', 'Unknown error')})")
                    continue

                mean_lat = result['mean_latency']
                model_stats[model].append(mean_lat)

                print(f"\nâœ… {model}:")
                print(f"   í‰ê·  ì§€ì—°ì‹œê°„: {mean_lat:.3f}ì´ˆ")
                print(f"   ì¤‘ì•™ê°’: {result['median_latency']:.3f}ì´ˆ")
                print(f"   ìµœì†Œ/ìµœëŒ€: {result['min_latency']:.3f}ì´ˆ / {result['max_latency']:.3f}ì´ˆ")
                print(f"   í‘œì¤€í¸ì°¨: {result['stdev_latency']:.3f}ì´ˆ")
                print(f"   ì‹¤íŒ¨: {result['errors']}/{result['runs']}íšŒ")
                print(f"   ìƒ˜í”Œ ì‘ë‹µ: {result['sample_response']}...")

        # ì „ì²´ í‰ê·  ë¹„êµ
        print(f"\n\n" + "=" * 80)
        print(" ğŸ† ì „ì²´ í‰ê·  ë¹„êµ ".center(80, "="))
        print("=" * 80)

        sorted_models = sorted(
            model_stats.items(),
            key=lambda x: mean(x[1]) if x[1] else float('inf')
        )

        for rank, (model, latencies) in enumerate(sorted_models, 1):
            if not latencies:
                print(f"\n{rank}. {model}: ë°ì´í„° ì—†ìŒ")
                continue

            avg_latency = mean(latencies)
            total_time = sum(latencies)

            print(f"\n{rank}. {model}:")
            print(f"   í‰ê·  ì§€ì—°ì‹œê°„: {avg_latency:.3f}ì´ˆ")
            print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
            print(f"   í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(latencies)}ê°œ")

            # 1ë“± ëŒ€ë¹„ ì†ë„ ê³„ì‚°
            if rank == 1:
                baseline = avg_latency
                print(f"   ğŸ¥‡ ê°€ì¥ ë¹ ë¦„!")
            else:
                slower_by = (avg_latency / baseline - 1) * 100
                print(f"   ğŸ“Š 1ë“±ë³´ë‹¤ {slower_by:.1f}% ëŠë¦¼")

        print("\n" + "=" * 80)

    def export_csv(self, filename: str = "benchmark_results.csv"):
        """ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"""
        import csv

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'Test Case',
                'Model',
                'Mean Latency (s)',
                'Median Latency (s)',
                'Min Latency (s)',
                'Max Latency (s)',
                'Stdev (s)',
                'Errors',
                'Runs'
            ])

            for test_name, test_results in self.results.items():
                for model, result in test_results.items():
                    if not result.get('success'):
                        continue

                    writer.writerow([
                        test_name,
                        model,
                        f"{result['mean_latency']:.3f}",
                        f"{result['median_latency']:.3f}",
                        f"{result['min_latency']:.3f}",
                        f"{result['max_latency']:.3f}",
                        f"{result['stdev_latency']:.3f}",
                        result['errors'],
                        result['runs']
                    ])

        print(f"\nğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="LLM ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument(
        "--models",
        nargs="+",
        default=["llama3", "gpt-4o"],
        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ê¸°ë³¸ê°’: llama3 gpt-4o)"
    )
    parser.add_argument(
        "--runs",
        type=int,
        default=3,
        help="ê° í…ŒìŠ¤íŠ¸ë‹¹ ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)"
    )
    parser.add_argument(
        "--export",
        action="store_true",
        help="ê²°ê³¼ë¥¼ CSVë¡œ ë‚´ë³´ë‚´ê¸°"
    )
    parser.add_argument(
        "--skip-api-check",
        action="store_true",
        help="API í‚¤ ì²´í¬ ê±´ë„ˆë›°ê¸° (í…ŒìŠ¤íŠ¸ìš©, OpenAI í˜¸ì¶œ ì‹¤íŒ¨ ì˜ˆìƒ)"
    )

    args = parser.parse_args()

    # OpenAI ì‚¬ìš© ì‹œ API í‚¤ ì²´í¬
    if any("gpt" in model or "openai" in model for model in args.models):
        if not os.getenv("OPENAI_API_KEY"):
            if not args.skip_api_check:
                print("\nâŒ ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("\ní•´ê²° ë°©ë²•:")
                print("  1. OpenAI API í‚¤ ì„¤ì •:")
                print("     export OPENAI_API_KEY=sk-proj-...")
                print("\n  2. ë˜ëŠ” --skip-api-check í”Œë˜ê·¸ ì‚¬ìš© (OpenAI í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì˜ˆìƒ):")
                print("     python benchmark_llm.py --models llama3 gpt-4o --skip-api-check")
                print("\n  3. ë˜ëŠ” Ollama ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸:")
                print("     python benchmark_llm.py --models llama3\n")
                sys.exit(1)
            else:
                print("\nâš ï¸  ê²½ê³ : OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   --skip-api-check í”Œë˜ê·¸ë¡œ ì¸í•´ ê³„ì† ì‹¤í–‰í•˜ì§€ë§Œ,")
                print("   OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤íŒ¨í•  ê²ƒì…ë‹ˆë‹¤.\n")

    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = LLMBenchmark()

    try:
        benchmark.run_benchmark(
            models=args.models,
            num_runs=args.runs
        )

        # ê²°ê³¼ ì¶œë ¥
        benchmark.print_summary()

        # CSV ë‚´ë³´ë‚´ê¸°
        if args.export:
            benchmark.export_csv()

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!\n")


if __name__ == "__main__":
    main()
