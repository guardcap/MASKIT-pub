"""
LLM Provider Factory
Ollamaì™€ OpenAIë¥¼ ìë™ìœ¼ë¡œ ì „í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜
"""
import os
from typing import Optional


def get_llm(model: str = "llama3", temperature: float = 0.0):
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜

    ëª¨ë¸ëª… í˜•ì‹:
    - Ollama: "llama3", "ollama:llama3", "ollama:mistral"
    - OpenAI: "gpt-4", "gpt-3.5-turbo", "openai:gpt-4"

    Args:
        model: ëª¨ë¸ëª… (provider:model í˜•ì‹ ë˜ëŠ” ë‹¨ìˆœ ëª¨ë¸ëª…)
        temperature: ìƒì„± ì˜¨ë„ (0.0 = ê²°ì •ì , 1.0 = ì°½ì˜ì )

    Returns:
        ChatOllama ë˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤

    Examples:
        >>> llm = get_llm("llama3")  # Ollama
        >>> llm = get_llm("gpt-4")   # OpenAI
        >>> llm = get_llm("openai:gpt-4o")  # ëª…ì‹œì  provider
    """

    # Providerì™€ ëª¨ë¸ëª… ë¶„ë¦¬
    if ":" in model:
        provider, model_name = model.split(":", 1)
    else:
        # Providerê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ìë™ ê°ì§€
        if model.startswith("gpt-") or model.startswith("o1-"):
            provider = "openai"
            model_name = model
        else:
            provider = "ollama"
            model_name = model

    provider = provider.lower()

    if provider == "openai":
        return _get_openai_llm(model_name, temperature)
    elif provider == "ollama":
        return _get_ollama_llm(model_name, temperature)
    else:
        raise ValueError(
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}\n"
            f"ì§€ì› provider: 'openai', 'ollama'"
        )


def _get_openai_llm(model: str, temperature: float):
    """OpenAI ChatGPT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    try:
        from langchain_openai import ChatOpenAI
    except ImportError:
        raise ImportError(
            "OpenAIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ langchain-openai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
            "ì„¤ì¹˜: pip install langchain-openai"
        )

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            "export OPENAI_API_KEY=sk-..."
        )

    print(f"   - OpenAI ëª¨ë¸ ì‚¬ìš©: {model}")

    return ChatOpenAI(
        model=model,
        temperature=temperature,
        openai_api_key=api_key
    )


def _get_ollama_llm(model: str, temperature: float):
    """Ollama ë¡œì»¬ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    try:
        from langchain_ollama import ChatOllama
    except ImportError:
        raise ImportError(
            "Ollamaë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ langchain-ollama íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
            "ì„¤ì¹˜: pip install langchain-ollama"
        )

    print(f"   - Ollama ëª¨ë¸ ì‚¬ìš©: {model}")

    # Ollama ì„œë²„ URL (ê¸°ë³¸ê°’: localhost:11434)
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

    return ChatOllama(
        model=model,
        temperature=temperature,
        base_url=base_url
    )


# ì§€ì› ëª¨ë¸ ëª©ë¡ (ì°¸ê³ ìš©)
SUPPORTED_MODELS = {
    "ollama": [
        "llama3",
        "llama3:70b",
        "mistral",
        "mixtral",
        "codellama",
        "phi",
    ],
    "openai": [
        "gpt-4",
        "gpt-4-turbo",
        "gpt-4o",
        "gpt-3.5-turbo",
        "o1-preview",
        "o1-mini",
    ]
}


def list_supported_models():
    """ì§€ì›í•˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
    print("\nğŸ“‹ ì§€ì›í•˜ëŠ” LLM ëª¨ë¸:")
    print("\nğŸ”· Ollama (ë¡œì»¬):")
    for model in SUPPORTED_MODELS["ollama"]:
        print(f"   - {model}")
    print("\nğŸ”· OpenAI (API):")
    for model in SUPPORTED_MODELS["openai"]:
        print(f"   - {model}")
    print()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    list_supported_models()

    # ì‚¬ìš© ì˜ˆì‹œ
    print("ì‚¬ìš© ì˜ˆì‹œ:")
    print('  llm = get_llm("llama3")       # Ollama')
    print('  llm = get_llm("gpt-4")        # OpenAI')
    print('  llm = get_llm("openai:gpt-4") # ëª…ì‹œì ')
