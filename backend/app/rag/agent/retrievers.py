"""
ê°œì„ ëœ Hybrid Retriever
- ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œ ìš°ì„  ê²€ìƒ‰
- BM25 + Vector í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- ê³„ì¸µì  ì²­í‚¹ì„ ê³ ë ¤í•œ ê²€ìƒ‰
"""
import os
import json
import pickle
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from konlpy.tag import Okt
from rank_bm25 import BM25Okapi
import torch
from typing import List, Dict, Any, Optional
from collections import defaultdict


class HybridRetriever:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° - BM25 + Vector Search"""

    def __init__(self, index_base_path='./data/staging'):
        print("âœ… Hybrid Retriever ì´ˆê¸°í™” ì¤‘...")
        self.index_base_path = index_base_path
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €
        self.model = SentenceTransformer('upskyy/e5-base-korean', device=device)
        self.tokenizer = Okt()

        # ChromaDB í´ë¼ì´ì–¸íŠ¸
        chroma_db_path = os.path.join(index_base_path, 'chroma_db')
        self.client = chromadb.PersistentClient(
            path=chroma_db_path,
            settings=Settings(anonymized_telemetry=False)
        )

        # Application Guides ë¡œë“œ (JSONL)
        self.guides = []
        self.guides_corpus = []
        self.guides_bm25 = None
        self._load_application_guides()

        # A_cases BM25 ì¸ë±ìŠ¤ ë¡œë“œ
        self.bm25_A = None
        self.documents_A = []
        self.meta_index_A = {}
        self._load_a_cases_index()

        print("ğŸ‰ Hybrid Retriever ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_application_guides(self):
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œ ë¡œë“œ ë° BM25 ì¸ë±ìŠ¤ ìƒì„±"""
        guides_path = os.path.join(self.index_base_path, 'application_guides.jsonl')

        if not os.path.exists(guides_path):
            print(f"âš ï¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {guides_path}")
            return

        try:
            with open(guides_path, 'r', encoding='utf-8') as f:
                for line in f:
                    guide = json.loads(line.strip())
                    self.guides.append(guide)

                    # ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„±: scenario + interpretation + keywords
                    search_text = f"{guide['scenario']} {guide['interpretation']} {' '.join(guide['keywords'])}"
                    self.guides_corpus.append(search_text)

            # BM25 ì¸ë±ìŠ¤ ìƒì„±
            if self.guides_corpus:
                tokenized_corpus = [self.tokenizer.morphs(text) for text in self.guides_corpus]
                self.guides_bm25 = BM25Okapi(tokenized_corpus)
                print(f"   - ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œ {len(self.guides)}ê°œ ë¡œë“œ ë° BM25 ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            print(f"   - ğŸš¨ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def _load_a_cases_index(self):
        """A_cases BM25 ì¸ë±ìŠ¤ ë¡œë“œ"""
        index_path = os.path.join(self.index_base_path, 'bm25/A_cases_bm25.pkl')
        try:
            with open(index_path, 'rb') as f:
                data = pickle.load(f)
                self.bm25_A = data['bm25']
                self.documents_A = data['documents']
                self.meta_index_A = data['meta_index']
            print(f"   - A_cases ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(self.documents_A)}ê°œ ë¬¸ì„œ).")
        except FileNotFoundError:
            print(f"   - âš ï¸ A_cases BM25 ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
        except Exception as e:
            print(f"   - ğŸš¨ A_cases ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def search_application_guides(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        top_k: int = 3,
        use_hybrid: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ì• í”Œë¦¬ì¼€ì´ì…˜ ê°€ì´ë“œ ê²€ìƒ‰ (ìµœìš°ì„ )
        - BM25ì™€ Vector ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        """
        if not self.guides:
            return []

        results = []

        # 1. BM25 ê²€ìƒ‰
        bm25_scores = {}
        if self.guides_bm25 and use_hybrid:
            tokenized_query = self.tokenizer.morphs(query)
            bm25_raw_scores = self.guides_bm25.get_scores(tokenized_query)
            bm25_scores = {i: score for i, score in enumerate(bm25_raw_scores)}

        # 2. Vector ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)
        query_embedding = self.model.encode(query)
        guide_embeddings = self.model.encode(self.guides_corpus)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
        vector_scores = cosine_similarity([query_embedding], guide_embeddings)[0]

        # 3. í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚° (RRF - Reciprocal Rank Fusion ê°„ì†Œí™” ë²„ì „)
        combined_scores = {}
        for i in range(len(self.guides)):
            bm25_score = bm25_scores.get(i, 0) if use_hybrid else 0
            vector_score = vector_scores[i]

            # ê°€ì¤‘ì¹˜ ì¡°í•© (Vectorì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
            combined_scores[i] = 0.3 * bm25_score + 0.7 * vector_score

            # Context í•„í„°ë§ (ì˜µì…˜)
            if context:
                guide = self.guides[i]
                guide_context = guide.get('context', {})

                # ë°œì‹ ì/ìˆ˜ì‹ ì íƒ€ì… ì¼ì¹˜ ì‹œ ë³´ë„ˆìŠ¤
                if context.get('sender_type') == guide_context.get('sender_type'):
                    combined_scores[i] *= 1.2
                if context.get('receiver_type') == guide_context.get('receiver_type'):
                    combined_scores[i] *= 1.2

        # 4. ìƒìœ„ Kê°œ ì„ íƒ
        sorted_indices = sorted(combined_scores.keys(), key=lambda x: combined_scores[x], reverse=True)[:top_k]

        for idx in sorted_indices:
            results.append({
                'guide_id': self.guides[idx]['guide_id'],
                'score': combined_scores[idx],
                'source': 'application_guides',
                'content': self.guides[idx],
                'metadata': {
                    'scenario': self.guides[idx]['scenario'],
                    'actionable_directive': self.guides[idx]['actionable_directive']
                }
            })

        return results

    def search_laws_and_policies(self, query: str, top_k: int = 5) -> Dict[str, List[Dict[str, Any]]]:
        """
        ë²•ë¥  ë° ì •ì±… ê²€ìƒ‰ (Vector Search)
        """
        results = {'laws': [], 'policies': []}

        try:
            query_embedding = self.model.encode(query).tolist()

            # C_laws ê²€ìƒ‰
            try:
                collection_laws = self.client.get_collection('C_laws')
                laws_results = collection_laws.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    include=["metadatas", "documents", "distances"]
                )

                for i, doc_id in enumerate(laws_results['ids'][0]):
                    results['laws'].append({
                        'id': doc_id,
                        'score': 1 - laws_results['distances'][0][i],
                        'source': 'C_laws',
                        'content': laws_results['documents'][0][i],
                        'metadata': laws_results['metadatas'][0][i]
                    })
            except Exception as e:
                print(f"âš ï¸ C_laws ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

            # B_policies ê²€ìƒ‰
            try:
                collection_policies = self.client.get_collection('B_policies')
                policies_results = collection_policies.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    include=["metadatas", "documents", "distances"]
                )

                for i, doc_id in enumerate(policies_results['ids'][0]):
                    results['policies'].append({
                        'id': doc_id,
                        'score': 1 - policies_results['distances'][0][i],
                        'source': 'B_policies',
                        'content': policies_results['documents'][0][i],
                        'metadata': policies_results['metadatas'][0][i]
                    })
            except Exception as e:
                print(f"âš ï¸ B_policies ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"ğŸš¨ ë²•ë¥ /ì •ì±… ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

        return results

    def search_cases(
        self,
        query: str,
        filters: Optional[Dict[str, Any]] = None,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        A_cases ê²€ìƒ‰ (BM25 with metadata filtering)
        """
        if not self.bm25_A:
            return []

        # í•„í„°ë§
        candidate_indices = set(range(len(self.documents_A)))
        if filters:
            for field, value in filters.items():
                if field in self.meta_index_A and value in self.meta_index_A[field]:
                    candidate_indices.intersection_update(self.meta_index_A[field][value])
                else:
                    return []

        if not candidate_indices:
            return []

        # BM25 ê²€ìƒ‰
        tokenized_query = self.tokenizer.morphs(query)
        doc_scores = self.bm25_A.get_scores(tokenized_query)

        candidate_scores = {idx: doc_scores[idx] for idx in candidate_indices}
        sorted_indices = sorted(candidate_scores, key=candidate_scores.get, reverse=True)[:top_k]

        results = []
        for idx in sorted_indices:
            doc = self.documents_A[idx]
            results.append({
                'id': doc.get('case_id', 'N/A'),
                'score': candidate_scores[idx],
                'source': 'A_cases',
                'content': doc.get('before_text', '') + ' -> ' + doc.get('after_text', ''),
                'metadata': doc
            })

        return results
