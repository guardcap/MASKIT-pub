"""
ì‹¤ë¬´ ê°€ì´ë“œë¼ì¸ PDF â†’ êµ¬ì¡°í™”ëœ VectorDB íŒŒì´í”„ë¼ì¸
OpenAI API + Zerox ê¸°ë°˜, ëŒ€ìš©ëŸ‰(50MB+) PDF ì§€ì›
"""

import asyncio
import json
import os
from pathlib import Path
from typing import List, Dict, Optional
import re
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# PDF ì²˜ë¦¬
import fitz  # PyMuPDF for large PDFs
from pyzerox import zerox
from openai import AsyncOpenAI

# ê²€ì¦ ë° ìœ í‹¸ë¦¬í‹°
from pydantic import BaseModel, ValidationError, Field
from tqdm import tqdm
import hashlib


class GuideContext(BaseModel):
    """ì´ë©”ì¼ ì»¨í…ìŠ¤íŠ¸"""
    sender_type: Optional[str] = Field(None, description="internal, external_customer, partner, regulatory")
    receiver_type: Optional[str] = Field(None, description="internal, external_customer, partner, regulatory")
    email_purpose: Optional[str] = Field(None, description="ê²¬ì ì„œ, ë¬¸ì˜, ë³´ê³  ë“±")
    pii_types: Optional[List[str]] = Field(default_factory=list, description="phone, ssn, account, address, email, name")


class GuideExample(BaseModel):
    """ê°€ì´ë“œ ì˜ˆì‹œ"""
    case_description: str
    masking_decision: str  # mask, no_mask, partial_mask
    reasoning: str


class ApplicationGuide(BaseModel):
    """ì‹¤ë¬´ ê°€ì´ë“œë¼ì¸ ìŠ¤í‚¤ë§ˆ"""
    guide_id: str
    source_authority: str  # ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ, ê³µì •ê±°ë˜ìœ„ì›íšŒ, KISA ë“±
    source_document: str
    publish_date: Optional[str] = None
    document_url: Optional[str] = None
    scenario: str
    context: Optional[GuideContext] = None
    interpretation: str
    actionable_directive: str
    keywords: List[str]
    related_law_ids: List[str] = Field(default_factory=list)
    examples: List[GuideExample] = Field(default_factory=list)
    confidence_score: float = Field(default=0.8, ge=0.0, le=1.0)
    reviewed: bool = False
    reviewer_notes: Optional[str] = None


class LargePDFProcessor:
    """ëŒ€ìš©ëŸ‰ PDF ìµœì í™” ì²˜ë¦¬ê¸°"""

    def __init__(self, max_pages_per_batch: int = 10):
        self.max_pages_per_batch = max_pages_per_batch

    def get_pdf_info(self, pdf_path: str) -> Dict:
        """PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        doc = fitz.open(pdf_path)
        info = {
            "page_count": len(doc),
            "file_size_mb": Path(pdf_path).stat().st_size / (1024 * 1024),
            "metadata": doc.metadata,
        }
        doc.close()
        return info

    def split_pdf_by_pages(self, pdf_path: str, output_dir: Path) -> List[Path]:
        """ëŒ€ìš©ëŸ‰ PDFë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë¶„í• """
        doc = fitz.open(pdf_path)
        total_pages = len(doc)

        output_dir.mkdir(parents=True, exist_ok=True)
        split_files = []

        for i in range(0, total_pages, self.max_pages_per_batch):
            end_page = min(i + self.max_pages_per_batch, total_pages)

            # ìƒˆ PDF ìƒì„±
            new_doc = fitz.open()
            new_doc.insert_pdf(doc, from_page=i, to_page=end_page - 1)

            output_path = output_dir / f"batch_{i:04d}_{end_page:04d}.pdf"
            new_doc.save(output_path)
            new_doc.close()

            split_files.append(output_path)

        doc.close()
        return split_files

    def extract_text_fallback(self, pdf_path: str) -> str:
        """OCR ì‹¤íŒ¨ ì‹œ PyMuPDFë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()
        return text


class GuidelineProcessor:
    """OpenAI ê¸°ë°˜ ê°€ì´ë“œë¼ì¸ í”„ë¡œì„¸ì„œ"""

    def __init__(
        self,
        openai_api_key: str = None,
        model: str = None,
        vision_model: str = None,
        output_dir: str = "data/staging",  # ë³€ê²½: application_guides ì œê±°
        temp_dir: str = "data/temp/pdf_batches"
    ):
        # .envì—ì„œ ê¸°ë³¸ê°’ ë¡œë“œ
        api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment or .env file")

        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o")
        self.vision_model = vision_model or os.getenv("OPENAI_VISION_MODEL", "gpt-4o")
        self.output_dir = Path(output_dir)
        self.temp_dir = Path(temp_dir)

        batch_size = int(os.getenv("PDF_BATCH_SIZE", "10"))
        self.pdf_processor = LargePDFProcessor(max_pages_per_batch=batch_size)

        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (íŒŒì¼ëª… ê³ ìœ ì„± ë³´ì¥)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    async def process_pdf(self, pdf_path: str, authority: str = "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ") -> List[ApplicationGuide]:
        """PDF íŒŒì¼ì„ ê°€ì´ë“œë¼ì¸ ê°ì²´ë¡œ ë³€í™˜"""
        pdf_path_obj = Path(pdf_path)
        print(f"\n{'='*60}")
        print(f"Processing: {pdf_path_obj.name}")

        # PDF ì •ë³´ í™•ì¸
        pdf_info = self.pdf_processor.get_pdf_info(pdf_path)
        print(f"Pages: {pdf_info['page_count']}, Size: {pdf_info['file_size_mb']:.2f} MB")

        # ëŒ€ìš©ëŸ‰ PDF ì²˜ë¦¬ ì „ëµ
        if pdf_info['file_size_mb'] > 20 or pdf_info['page_count'] > 50:
            print("âš ï¸  Large PDF detected - using batch processing")
            return await self._process_large_pdf(pdf_path, pdf_path_obj, authority)
        else:
            return await self._process_standard_pdf(pdf_path, pdf_path_obj, authority)

    async def _process_standard_pdf(
        self,
        pdf_path: str,
        pdf_path_obj: Path,
        authority: str
    ) -> List[ApplicationGuide]:
        """í‘œì¤€ í¬ê¸° PDF ì²˜ë¦¬ (Zerox ì‚¬ìš©)"""
        print("ğŸ“„ Using Zerox for OCR + Vision processing...")

        try:
            # Zeroxë¡œ PDF â†’ Markdown ë³€í™˜
            result = await zerox(
                file_path=pdf_path,
                model=self.vision_model,
                output_dir=str(self.temp_dir),
                cleanup=True,  # ì„ì‹œ ì´ë¯¸ì§€ ìë™ ì‚­ì œ
            )

            markdown = result.get("content", "") if isinstance(result, dict) else str(result)

        except Exception as e:
            print(f"âŒ Zerox failed: {e}")
            print("ğŸ”„ Falling back to PyMuPDF text extraction...")
            markdown = self.pdf_processor.extract_text_fallback(pdf_path)

        # ê°€ì´ë“œë¼ì¸ êµ¬ì¡°í™”
        guides = await self._structure_document(
            markdown,
            source_document=pdf_path_obj.name,
            authority=authority
        )

        return guides

    async def _process_large_pdf(
        self,
        pdf_path: str,
        pdf_path_obj: Path,
        authority: str
    ) -> List[ApplicationGuide]:
        """ëŒ€ìš©ëŸ‰ PDF ë°°ì¹˜ ì²˜ë¦¬"""
        print("ğŸ”ª Splitting PDF into smaller batches...")

        batch_dir = self.temp_dir / f"batches_{pdf_path_obj.stem}"
        batch_files = self.pdf_processor.split_pdf_by_pages(pdf_path, batch_dir)

        print(f"âœ… Created {len(batch_files)} batches")

        all_guides = []

        # ë°°ì¹˜ë³„ ì²˜ë¦¬ (Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´ ì¶”ê°€)
        batch_delay = int(os.getenv("PDF_BATCH_DELAY", "3"))

        for i, batch_file in enumerate(tqdm(batch_files, desc="Processing batches")):
            try:
                # Rate Limit ë°©ì§€: ê° ë°°ì¹˜ ì‚¬ì´ì— ë”œë ˆì´
                if i > 0:
                    print(f"â¸ï¸  Waiting {batch_delay} seconds to avoid rate limit...")
                    await asyncio.sleep(batch_delay)

                # Zerox ì²˜ë¦¬
                result = await zerox(
                    file_path=str(batch_file),
                    model=self.vision_model,
                    output_dir=str(self.temp_dir / f"batch_{i}"),
                    cleanup=True,
                )

                markdown = result.get("content", "") if isinstance(result, dict) else str(result)

                # êµ¬ì¡°í™”
                guides = await self._structure_document(
                    markdown,
                    source_document=f"{pdf_path_obj.name} (batch {i+1}/{len(batch_files)})",
                    authority=authority
                )

                all_guides.extend(guides)

            except Exception as e:
                print(f"âš ï¸  Batch {i+1} failed: {e}")
                continue

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        import shutil
        shutil.rmtree(batch_dir, ignore_errors=True)

        return all_guides

    async def _structure_document(
        self,
        markdown: str,
        source_document: str,
        authority: str
    ) -> List[ApplicationGuide]:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ê°€ì´ë“œë¼ì¸ìœ¼ë¡œ êµ¬ì¡°í™”"""

        # Step 1: ì„¹ì…˜ ë¶„í• 
        sections = await self._extract_sections(markdown)
        print(f"ğŸ“‘ Extracted {len(sections)} sections")

        # Step 2: ê° ì„¹ì…˜ êµ¬ì¡°í™”
        guides = []
        for i, section in enumerate(sections):
            guide_data = await self._structure_section(section, source_document, authority)

            if guide_data:
                # guide_id ìƒì„±
                guide_data["guide_id"] = self._generate_guide_id(
                    authority,
                    source_document,
                    i
                )

                try:
                    guide = ApplicationGuide(**guide_data)
                    guides.append(guide)
                except ValidationError as e:
                    print(f"âš ï¸  Validation error for section {i}: {e}")
                    continue

        return guides

    async def _extract_sections(self, markdown: str) -> List[str]:
        """ë§ˆí¬ë‹¤ìš´ì„ ë…¼ë¦¬ì  ì„¹ì…˜ìœ¼ë¡œ ë¶„í• """

        # ê°„ë‹¨í•œ ê·œì¹™: í—¤ë”(#, ##)ë¡œ ë¶„í• 
        # ë” ì •êµí•œ ë°©ë²•: LLMì—ê²Œ ì˜ë¯¸ì  ë¶„í•  ìš”ì²­

        if len(markdown) < 500:
            return [markdown]

        # LLMìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
        prompt = f"""
ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ ë¬¸ì„œë¥¼ ë…ë¦½ì ì¸ ì‹¤ë¬´ ê°€ì´ë“œ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• í•˜ì„¸ìš”.
ê° ì„¹ì…˜ì€ í•˜ë‚˜ì˜ êµ¬ì²´ì ì¸ ìƒí™©/ì§€ì¹¨ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.

ë¬¸ì„œ:
{markdown[:8000]}  # í† í° ì œí•œ

JSON ë°°ì—´ë¡œ ë°˜í™˜:
["ì„¹ì…˜ 1 ì „ì²´ í…ìŠ¤íŠ¸", "ì„¹ì…˜ 2 ì „ì²´ í…ìŠ¤íŠ¸", ...]

ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì„¤ëª… ì—†ì´ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )

            content = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
            content = re.sub(r'^```json\s*', '', content)
            content = re.sub(r'\s*```$', '', content)

            sections = json.loads(content)
            return sections if isinstance(sections, list) else [markdown]

        except Exception as e:
            print(f"âš ï¸  Section splitting failed: {e}")
            # Fallback: í—¤ë” ê¸°ë°˜ ë¶„í• 
            return self._split_by_headers(markdown)

    def _split_by_headers(self, markdown: str) -> List[str]:
        """í—¤ë” ê¸°ë°˜ ë‹¨ìˆœ ë¶„í• """
        sections = re.split(r'\n#{1,3}\s+', markdown)
        return [s.strip() for s in sections if len(s.strip()) > 100]

    async def _structure_section(
        self,
        section: str,
        source_document: str,
        authority: str
    ) -> Optional[Dict]:
        """ì„¹ì…˜ì„ ìŠ¤í‚¤ë§ˆì— ë§ì¶° êµ¬ì¡°í™”"""

        prompt = f"""
ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ ì„¹ì…˜ì„ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.

ì„¹ì…˜ í…ìŠ¤íŠ¸:
{section[:4000]}

ë‹¤ìŒ í•„ë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
- scenario: ì ìš© ìƒí™© (100ì ì´ë‚´, ì˜ˆ: "ì™¸ë¶€ ê³ ê°ì´ ì œí’ˆ ë¬¸ì˜ë¥¼ ìœ„í•´ ì´ë©”ì¼ì„ ë³´ë‚¸ ê²½ìš°")
- context: {{
    "sender_type": "internal/external_customer/partner/regulatory ì¤‘ í•˜ë‚˜",
    "receiver_type": "internal/external_customer/partner/regulatory ì¤‘ í•˜ë‚˜",
    "email_purpose": "ê²¬ì ì„œ/ë¬¸ì˜/ë³´ê³  ë“±",
    "pii_types": ["phone", "ssn", "email", "name" ë“± ë°°ì—´]
  }}
- interpretation: ë²•ì  í•´ì„ ë° ë°°ê²½ ì„¤ëª…
- actionable_directive: ì‹¤í–‰ ì§€ì¹¨ (ë§ˆìŠ¤í‚¹ ì˜ˆì™¸/ë§ˆìŠ¤í‚¹ í•„ìˆ˜ ë“± ëª…í™•í•˜ê²Œ)
- keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë°°ì—´ (5-10ê°œ)
- related_law_ids: ê´€ë ¨ ë²•ë¥  ì¡°í•­ ID ë°°ì—´ (ì˜ˆ: ["ê°œì¸ì •ë³´ë³´í˜¸ë²•_ì œ15ì¡°_1í•­"])
- examples: [{{
    "case_description": "ì‚¬ë¡€ ì„¤ëª…",
    "masking_decision": "mask/no_mask/partial_mask",
    "reasoning": "íŒë‹¨ ê·¼ê±°"
  }}] (ì„ íƒì‚¬í•­)

JSON í¬ë§· (ì„¤ëª… ì—†ì´ JSONë§Œ):
{{
  "scenario": "...",
  "context": {{}},
  "interpretation": "...",
  "actionable_directive": "...",
  "keywords": [],
  "related_law_ids": [],
  "examples": []
}}

ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )

            content = response.choices[0].message.content.strip()

            # JSON ì¶”ì¶œ
            content = re.sub(r'^```json\s*', '', content)
            content = re.sub(r'\s*```$', '', content)

            guide_data = json.loads(content)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            guide_data["source_authority"] = authority
            guide_data["source_document"] = source_document
            guide_data["confidence_score"] = 0.8
            guide_data["reviewed"] = False

            return guide_data

        except Exception as e:
            print(f"âš ï¸  Section structuring failed: {e}")
            return None

    def _generate_guide_id(self, authority: str, source_doc: str, index: int) -> str:
        """ê³ ìœ  guide_id ìƒì„±"""
        # ê¸°ê´€ëª… ì•½ì–´
        authority_map = {
            "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": "PIPC",
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ": "FTC",
            "KISA": "KISA",
            "ê¸ˆìœµê°ë…ì›": "FSS",
            "ë°©ì†¡í†µì‹ ìœ„ì›íšŒ": "KCC",
        }

        authority_code = authority_map.get(authority, "UNK")

        # ë¬¸ì„œëª… í•´ì‹œ
        doc_hash = hashlib.md5(source_doc.encode()).hexdigest()[:6]

        # ë‚ ì§œ
        date_str = datetime.now().strftime("%Y%m")

        return f"GUIDE-{authority_code}-{date_str}-{doc_hash}-{index:03d}"

    async def save_guides(self, guides: List[ApplicationGuide], suffix: str = "raw"):
        """
        ê°€ì´ë“œë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…)

        Args:
            guides: ì €ì¥í•  ê°€ì´ë“œ ë¦¬ìŠ¤íŠ¸
            suffix: íŒŒì¼ëª… ì ‘ë¯¸ì‚¬ (raw, unique, review ë“±)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª… ìƒì„±
        filename = f"application_guides_{self.timestamp}_{suffix}.jsonl"
        output_file = self.output_dir / filename

        with open(output_file, "w", encoding="utf-8") as f:
            for guide in guides:
                f.write(guide.model_dump_json(exclude_none=True) + "\n")

        print(f"\nâœ… Saved {len(guides)} guides to {output_file}")
        return output_file

    def validate_guides(self, guides: List[ApplicationGuide]) -> Dict[str, List[ApplicationGuide]]:
        """ê°€ì´ë“œ ê²€ì¦ ë° ë¶„ë¥˜"""
        valid = []
        needs_review = []

        for guide in guides:
            if guide.confidence_score < 0.7:
                needs_review.append(guide)
            elif not guide.scenario or not guide.actionable_directive:
                needs_review.append(guide)
            else:
                valid.append(guide)

        return {
            "valid": valid,
            "needs_review": needs_review,
        }


async def main():
    """ë©”ì¸ ì‹¤í–‰"""

    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (.envì—ì„œ ìë™ ë¡œë“œ)
    try:
        processor = GuidelineProcessor()
        print(f"âœ… Using model: {processor.model}")
        print(f"âœ… Using vision model: {processor.vision_model}")
    except ValueError as e:
        print(f"âŒ Configuration error: {e}")
        print("\nPlease ensure:")
        print("1. .env file exists in the project root")
        print("2. OPENAI_API_KEY is set in .env")
        print("\nOr run: export OPENAI_API_KEY='sk-...'")
        return

    # PDF íŒŒì¼ ì°¾ê¸°
    pdf_dir = Path("guardcap-rag/data/raw_guidelines")
    if not pdf_dir.exists():
        pdf_dir = Path("data/raw_guidelines")

    pdf_files = list(pdf_dir.glob("*.pdf"))

    if not pdf_files:
        print(f"âŒ No PDF files found in {pdf_dir}")
        print("Please add PDF files to data/raw_guidelines/")
        return

    print(f"\nğŸ” Found {len(pdf_files)} PDF files")

    # ì²˜ë¦¬í•  íŒŒì¼ ê°œìˆ˜ ì œí•œ (í…ŒìŠ¤íŠ¸ìš©)
    max_files = int(os.getenv("MAX_PDF_FILES", "5"))
    pdf_files = pdf_files[:max_files]

    all_guides = []

    # PDF ì²˜ë¦¬
    for pdf_file in pdf_files:
        try:
            guides = await processor.process_pdf(
                str(pdf_file),
                authority="ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ"  # ê¸°ë³¸ê°’, íŒŒì¼ëª…ìœ¼ë¡œ ìë™ ê°ì§€ ê°€ëŠ¥
            )
            all_guides.extend(guides)

        except Exception as e:
            print(f"âŒ Failed to process {pdf_file.name}: {e}")
            continue

    print(f"\n{'='*60}")
    print(f"ğŸ“Š Total guides extracted: {len(all_guides)}")

    # ê²€ì¦
    validation_result = processor.validate_guides(all_guides)
    valid = validation_result["valid"]
    needs_review = validation_result["needs_review"]

    print(f"âœ… Valid: {len(valid)}")
    print(f"âš ï¸  Needs review: {len(needs_review)}")

    # ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…)
    saved_files = []

    if valid:
        file_path = await processor.save_guides(valid, "raw")
        saved_files.append(str(file_path))

    if needs_review:
        file_path = await processor.save_guides(needs_review, "review")
        saved_files.append(str(file_path))

    print("\nâœ¨ Processing complete!")
    print(f"\nğŸ“ Saved files:")
    for file in saved_files:
        print(f"  - {file}")


if __name__ == "__main__":
    asyncio.run(main())
