"""
ê°€ì´ë“œë¼ì¸ ê²€ì¦ ë° ì¤‘ë³µ ì œê±° ìœ í‹¸ë¦¬í‹°
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI
import os
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class GuideValidator:
    """ê°€ì´ë“œë¼ì¸ ê²€ì¦ ë° ì¤‘ë³µ ì œê±°"""

    def __init__(self, openai_api_key: str = None):
        api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment or .env file")
        self.client = OpenAI(api_key=api_key)
        self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    def load_guides(self, jsonl_path: str) -> List[Dict]:
        """ë‹¨ì¼ JSONL íŒŒì¼ ë¡œë“œ"""
        guides = []
        with open(jsonl_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():  # ë¹ˆ ì¤„ ë¬´ì‹œ
                    guides.append(json.loads(line))
        return guides

    def load_all_guides_from_directory(self, directory: str, pattern: str = "application_guides_*_raw.jsonl") -> List[Dict]:
        """
        ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  JSONL íŒŒì¼ ë¡œë“œ

        Args:
            directory: ê²€ìƒ‰í•  ë””ë ‰í† ë¦¬
            pattern: íŒŒì¼ëª… íŒ¨í„´ (glob)

        Returns:
            ëª¨ë“  íŒŒì¼ì˜ ê°€ì´ë“œë¥¼ í•©ì¹œ ë¦¬ìŠ¤íŠ¸
        """
        dir_path = Path(directory)
        if not dir_path.exists():
            return []

        all_guides = []
        jsonl_files = sorted(dir_path.glob(pattern))

        if not jsonl_files:
            # Fallback: ëª¨ë“  application_guides_*.jsonl íŒŒì¼
            jsonl_files = sorted(dir_path.glob("application_guides_*.jsonl"))

        print(f"ğŸ“‚ Found {len(jsonl_files)} JSONL files in {directory}")

        for jsonl_file in jsonl_files:
            print(f"  - Loading: {jsonl_file.name}")
            guides = self.load_guides(str(jsonl_file))
            all_guides.extend(guides)

        return all_guides

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """OpenAI Embedding ìƒì„±"""
        embeddings = []

        # ë°°ì¹˜ ì²˜ë¦¬ (ìµœëŒ€ 2048ê°œ)
        batch_size = 100
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=batch
            )

            embeddings.extend([e.embedding for e in response.data])

        return np.array(embeddings)

    def remove_duplicates(
        self,
        guides: List[Dict],
        similarity_threshold: float = 0.85
    ) -> Tuple[List[Dict], List[Tuple[int, int, float]]]:
        """
        Embedding ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°

        Returns:
            - unique_guides: ì¤‘ë³µ ì œê±°ëœ ê°€ì´ë“œ
            - duplicates: (index1, index2, similarity) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ” Checking {len(guides)} guides for duplicates...")

        # ê²€ìƒ‰ í…ìŠ¤íŠ¸ ìƒì„±
        texts = [
            f"{g.get('scenario', '')} {g.get('actionable_directive', '')}"
            for g in guides
        ]

        # Embedding ìƒì„±
        print("ğŸ“Š Generating embeddings...")
        embeddings = self.get_embeddings(texts)

        # ìœ ì‚¬ë„ ê³„ì‚°
        print("ğŸ”¢ Computing similarity matrix...")
        similarity_matrix = cosine_similarity(embeddings)

        # ì¤‘ë³µ ê°ì§€
        to_remove = set()
        duplicate_pairs = []

        for i in range(len(guides)):
            for j in range(i + 1, len(guides)):
                sim = similarity_matrix[i][j]

                if sim > similarity_threshold:
                    duplicate_pairs.append((i, j, sim))
                    to_remove.add(j)  # ë‚˜ì¤‘ ê²ƒ ì œê±°

        # ì¤‘ë³µ ì œê±°
        unique_guides = [g for i, g in enumerate(guides) if i not in to_remove]

        print(f"âœ… Found {len(duplicate_pairs)} duplicate pairs")
        print(f"âœ… Removed {len(to_remove)} duplicates")
        print(f"âœ… Remaining: {len(unique_guides)} unique guides")

        return unique_guides, duplicate_pairs

    def create_review_queue(
        self,
        guides: List[Dict],
        min_confidence: float = 0.7
    ) -> List[Dict]:
        """íœ´ë¨¼ ë¦¬ë·°ê°€ í•„ìš”í•œ ê°€ì´ë“œ ì¶”ì¶œ"""

        review_needed = []

        for guide in guides:
            needs_review = False
            reasons = []

            # ì‹ ë¢°ë„ ë‚®ìŒ
            if guide.get("confidence_score", 1.0) < min_confidence:
                needs_review = True
                reasons.append(f"Low confidence: {guide.get('confidence_score'):.2f}")

            # í•„ìˆ˜ í•„ë“œ ëˆ„ë½
            if not guide.get("scenario"):
                needs_review = True
                reasons.append("Missing scenario")

            if not guide.get("actionable_directive"):
                needs_review = True
                reasons.append("Missing actionable_directive")

            # í‚¤ì›Œë“œ ë¶€ì¡±
            if len(guide.get("keywords", [])) < 3:
                needs_review = True
                reasons.append(f"Few keywords: {len(guide.get('keywords', []))}")

            if needs_review:
                review_item = guide.copy()
                review_item["review_reasons"] = reasons
                review_needed.append(review_item)

        return review_needed

    def save_review_queue(self, review_items: List[Dict], output_path: str):
        """ë¦¬ë·° íë¥¼ CSVë¡œ ì €ì¥"""
        import pandas as pd

        rows = []
        for item in review_items:
            rows.append({
                "guide_id": item.get("guide_id"),
                "scenario": item.get("scenario", "")[:100],
                "actionable_directive": item.get("actionable_directive", "")[:100],
                "confidence": item.get("confidence_score", "N/A"),
                "reasons": ", ".join(item.get("review_reasons", [])),
                "source": item.get("source_document", ""),
            })

        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… Review queue saved to {output_path}")

    def save_guides(self, guides: List[Dict], output_dir: str, suffix: str):
        """
        JSONL ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…)

        Args:
            guides: ì €ì¥í•  ê°€ì´ë“œ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            suffix: íŒŒì¼ëª… ì ‘ë¯¸ì‚¬ (unique, review ë“±)

        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        filename = f"application_guides_{self.timestamp}_{suffix}.jsonl"
        output_path = Path(output_dir) / filename

        with open(output_path, "w", encoding="utf-8") as f:
            for guide in guides:
                f.write(json.dumps(guide, ensure_ascii=False) + "\n")

        print(f"âœ… Saved {len(guides)} guides to {output_path}")
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰"""

    # ê²€ì¦ì ì´ˆê¸°í™” (.envì—ì„œ ìë™ ë¡œë“œ)
    try:
        validator = GuideValidator()
        print(f"âœ… Using embedding model: {validator.embedding_model}")
    except ValueError as e:
        print(f"âŒ Configuration error: {e}")
        print("\nPlease ensure:")
        print("1. .env file exists in the project root")
        print("2. OPENAI_API_KEY is set in .env")
        return

    # staging ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ê°€ì´ë“œ ë¡œë“œ
    staging_dirs = [
        "guardcap-rag/data/staging",
        "data/staging"
    ]

    guides = []
    for staging_dir in staging_dirs:
        if Path(staging_dir).exists():
            guides = validator.load_all_guides_from_directory(
                staging_dir,
                pattern="application_guides_*_raw.jsonl"
            )
            if guides:
                break

    if not guides:
        print("âŒ No guide files found in staging directory!")
        print("\nPlease run process_guidelines.py first to generate guide files.")
        return

    print(f"\nğŸ“Š Total loaded: {len(guides)} guides")

    # ì¤‘ë³µ ì œê±° (.env ì„¤ì • ì‚¬ìš©)
    similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.85"))
    unique_guides, duplicates = validator.remove_duplicates(
        guides,
        similarity_threshold=similarity_threshold
    )

    # ë¦¬ë·° í ìƒì„± (.env ì„¤ì • ì‚¬ìš©)
    min_confidence = float(os.getenv("MIN_CONFIDENCE", "0.7"))
    review_needed = validator.create_review_queue(
        unique_guides,
        min_confidence=min_confidence
    )

    print(f"\nğŸ“Š Summary:")
    print(f"  Original: {len(guides)}")
    print(f"  Unique: {len(unique_guides)}")
    print(f"  Needs Review: {len(review_needed)}")

    # ì €ì¥ (staging ë””ë ‰í† ë¦¬ì— íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª…)
    output_dir = "data/staging"
    if not Path(output_dir).exists():
        output_dir = "guardcap-rag/data/staging"

    saved_files = []

    # ì¤‘ë³µ ì œê±°ëœ ê°€ì´ë“œ ì €ì¥
    unique_file = validator.save_guides(unique_guides, output_dir, "unique")
    saved_files.append(str(unique_file))

    # ë¦¬ë·° í•„ìš” í•­ëª© ì €ì¥
    if review_needed:
        review_file = validator.save_guides(review_needed, output_dir, "review_needed")
        saved_files.append(str(review_file))

        # CSV ë¦¬ë·° í ì €ì¥
        csv_path = Path(output_dir) / f"review_queue_{validator.timestamp}.csv"
        validator.save_review_queue(review_needed, str(csv_path))
        saved_files.append(str(csv_path))

    # ì¤‘ë³µ ìŒ ì €ì¥
    if duplicates:
        report_path = Path(output_dir) / f"duplicates_report_{validator.timestamp}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "index1": i,
                        "index2": j,
                        "similarity": float(sim),
                        "guide1_id": guides[i].get("guide_id"),
                        "guide2_id": guides[j].get("guide_id"),
                    }
                    for i, j, sim in duplicates
                ],
                f,
                indent=2,
                ensure_ascii=False
            )
        saved_files.append(str(report_path))

    print("\nâœ¨ Validation complete!")
    print(f"\nğŸ“ Saved files:")
    for file in saved_files:
        print(f"  - {file}")


if __name__ == "__main__":
    main()
