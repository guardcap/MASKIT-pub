#!/usr/bin/env python3
"""
raw_guidelines ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDFë¥¼ ë°°ì¹˜ ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
- ìë™ìœ¼ë¡œ ë°œí–‰ ê¸°ê´€ ê°ì§€
- í° íŒŒì¼(>10MB)ì€ ë¶„í•  ì²˜ë¦¬
- ì§„í–‰ ìƒí™© ì¶”ì  ë° ì—ëŸ¬ ë¡œê¹…
"""

import asyncio
import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.guidelines.process_guidelines import GuidelineProcessor
from scripts.guidelines.build_guides_vectordb import GuidesVectorDBBuilder
from dotenv import load_dotenv
from tqdm import tqdm

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class AuthorityDetector:
    """íŒŒì¼ëª…ì—ì„œ ë°œí–‰ ê¸°ê´€ ê°ì§€"""

    AUTHORITY_KEYWORDS = {
        "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": ["ê°œì¸ì •ë³´", "ë³´í˜¸"],
        "ê¸ˆìœµë³´ì•ˆì›": ["ê¸ˆìœµ", "ë³´ì•ˆ"],
        "KISA": ["KISA", "ì •ë³´ë³´í˜¸"],
        "ê³µì •ê±°ë˜ìœ„ì›íšŒ": ["ê³µì •ê±°ë˜"],
        "ê¸ˆìœµìœ„ì›íšŒ": ["ê¸ˆìœµìœ„"],
        "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€": ["ê³¼í•™ê¸°ìˆ ", "í†µì‹ "],
        "í–‰ì •ì•ˆì „ë¶€": ["í–‰ì •ì•ˆì „", "ì •ë³´ë³´í˜¸"],
    }

    @staticmethod
    def detect(filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ë°œí–‰ ê¸°ê´€ ê°ì§€"""
        filename_lower = filename.lower()

        # ì •í™•í•œ ë§¤ì¹­
        for authority, keywords in AuthorityDetector.AUTHORITY_KEYWORDS.items():
            if all(kw.lower() in filename_lower for kw in keywords):
                return authority

        # ë¶€ë¶„ ë§¤ì¹­
        for authority, keywords in AuthorityDetector.AUTHORITY_KEYWORDS.items():
            if any(kw.lower() in filename_lower for kw in keywords):
                return authority

        # ê¸°ë³¸ê°’
        return "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ"


class RawPDFBatchProcessor:
    """raw_guidelines ë””ë ‰í† ë¦¬ì˜ PDF ë°°ì¹˜ ì²˜ë¦¬"""

    def __init__(self, raw_dir: str = "data/raw_guidelines"):
        self.raw_dir = Path(raw_dir)
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.vision_model = os.getenv("OPENAI_VISION_MODEL", "gpt-4o-mini")
        self.batch_delay = int(os.getenv("PDF_BATCH_DELAY", "3"))

        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not set")

        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.output_dir = Path("data/staging")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # íƒ€ì„ìŠ¤íƒí”„ (ëª¨ë“  íŒŒì¼ì— ë™ì¼í•˜ê²Œ ì‚¬ìš©)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        logger.info(f"Initialized batch processor")
        logger.info(f"Raw directory: {self.raw_dir}")
        logger.info(f"Output directory: {self.output_dir}")
        logger.info(f"Vision model: {self.vision_model}")

    def get_pdf_files(self) -> List[Tuple[Path, int]]:
        """raw_guidelines ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDF íŒŒì¼ ëª©ë¡"""
        if not self.raw_dir.exists():
            logger.error(f"Directory not found: {self.raw_dir}")
            return []

        pdf_files = []
        for pdf_file in sorted(self.raw_dir.glob("*.pdf")):
            file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
            pdf_files.append((pdf_file, file_size_mb))

        return pdf_files

    async def process_single_pdf(
        self,
        pdf_path: Path,
        file_size_mb: float,
        file_index: int,
        total_files: int
    ) -> Dict:
        """ë‹¨ì¼ PDF ì²˜ë¦¬"""
        logger.info(
            f"\n[{file_index}/{total_files}] Processing: {pdf_path.name} ({file_size_mb:.2f}MB)"
        )

        try:
            # ë°œí–‰ ê¸°ê´€ ê°ì§€
            authority = AuthorityDetector.detect(pdf_path.name)
            logger.info(f"  Detected authority: {authority}")

            # GuidelineProcessor ìƒì„±
            processor = GuidelineProcessor(
                openai_api_key=self.api_key,
                vision_model=self.vision_model,
                output_dir=str(self.output_dir)
            )

            # PDF ì²˜ë¦¬
            guides = await processor.process_pdf(str(pdf_path), authority)

            if not guides:
                logger.warning(f"  âš ï¸  No guides extracted from {pdf_path.name}")
                return {
                    "file": pdf_path.name,
                    "status": "no_data",
                    "guides_count": 0
                }

            logger.info(f"  âœ… Extracted {len(guides)} guides")

            # JSONL ì €ì¥ (ê³ ìœ  íŒŒì¼ëª…)
            async def save():
                return await processor.save_guides(
                    guides,
                    suffix=f"batch_{file_index:02d}"
                )

            output_file = await save()
            logger.info(f"  ğŸ’¾ Saved to: {Path(output_file).name}")

            return {
                "file": pdf_path.name,
                "status": "success",
                "guides_count": len(guides),
                "output_file": Path(output_file).name,
                "authority": authority,
                "file_size_mb": file_size_mb
            }

        except Exception as e:
            logger.error(f"  âŒ Error processing {pdf_path.name}: {str(e)}")
            return {
                "file": pdf_path.name,
                "status": "error",
                "error": str(e)
            }

    async def process_all_pdfs(self) -> List[Dict]:
        """ëª¨ë“  PDF ë°°ì¹˜ ì²˜ë¦¬"""
        pdf_files = self.get_pdf_files()

        if not pdf_files:
            logger.warning("No PDF files found in raw_guidelines directory")
            return []

        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ IDì™€ ì´ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì„¤ì •
        process_id = int(os.getenv("PROCESS_ID", "1"))
        total_processes = int(os.getenv("TOTAL_PROCESSES", "1"))

        # ì´ í”„ë¡œì„¸ìŠ¤ê°€ ì²˜ë¦¬í•  íŒŒì¼ í•„í„°ë§ (ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹)
        filtered_files = [
            (pdf_path, file_size_mb)
            for idx, (pdf_path, file_size_mb) in enumerate(pdf_files)
            if idx % total_processes == (process_id - 1)
        ]

        logger.info(f"\nProcess {process_id}/{total_processes}: Starting batch processing of {len(filtered_files)} PDF files...")
        logger.info(f"Total files in directory: {len(pdf_files)}")
        logger.info("=" * 80)

        results = []

        for file_idx, (pdf_path, file_size_mb) in enumerate(filtered_files, 1):
            # ì „ì²´ íŒŒì¼ ëª©ë¡ì—ì„œì˜ ì‹¤ì œ ì¸ë±ìŠ¤ ê³„ì‚°
            actual_idx = pdf_files.index((pdf_path, file_size_mb)) + 1

            # Rate Limit ë°©ì§€ (ì²« íŒŒì¼ ì œì™¸)
            if file_idx > 1:
                logger.info(f"Waiting {self.batch_delay} seconds before next file...")
                await asyncio.sleep(self.batch_delay)

            # ì²˜ë¦¬
            result = await self.process_single_pdf(pdf_path, file_size_mb, actual_idx, len(pdf_files))
            results.append(result)

        return results

    def build_vectordb(self) -> Dict:
        """ì²˜ë¦¬ëœ ëª¨ë“  ê°€ì´ë“œë¥¼ VectorDBì— ì¶”ê°€"""
        logger.info("\n" + "=" * 80)
        logger.info("Building VectorDB from all processed guides...")

        try:
            builder = GuidesVectorDBBuilder(
                openai_api_key=self.api_key,
                db_path="data/chromadb/application_guides"
            )

            # staging í´ë”ì˜ ëª¨ë“  íŒŒì¼ ë¡œë“œ
            staging_dir = Path("data/staging")
            all_guides = builder.load_all_guides_from_directory(
                str(staging_dir),
                pattern="application_guides_*.jsonl"
            )

            if not all_guides:
                logger.warning("No guides found in staging directory")
                return {
                    "status": "no_data",
                    "total_guides": 0
                }

            logger.info(f"Loading {len(all_guides)} guides from staging files")

            # VectorDB ì¶”ê°€
            builder.add_guides_to_db(all_guides, batch_size=50)

            logger.info(f"âœ… VectorDB updated with {len(all_guides)} guides")

            return {
                "status": "success",
                "total_guides": len(all_guides)
            }

        except Exception as e:
            logger.error(f"âŒ Error building VectorDB: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

    def save_results(self, results: List[Dict], vectordb_result: Dict):
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        summary = {
            "timestamp": self.timestamp,
            "total_files": len(results),
            "successful": sum(1 for r in results if r["status"] == "success"),
            "failed": sum(1 for r in results if r["status"] == "error"),
            "no_data": sum(1 for r in results if r["status"] == "no_data"),
            "total_guides_extracted": sum(r.get("guides_count", 0) for r in results),
            "vectordb": vectordb_result,
            "files": results
        }

        output_file = self.output_dir / f"batch_processing_report_{self.timestamp}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        logger.info(f"\nğŸ“Š Processing report saved to: {output_file.name}")

        return summary

    def print_summary(self, summary: Dict):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š BATCH PROCESSING SUMMARY")
        logger.info("=" * 80)
        logger.info(f"Total files processed: {summary['total_files']}")
        logger.info(f"âœ… Successful: {summary['successful']}")
        logger.info(f"âŒ Failed: {summary['failed']}")
        logger.info(f"âš ï¸  No data: {summary['no_data']}")
        logger.info(f"Total guides extracted: {summary['total_guides_extracted']}")
        logger.info(f"VectorDB status: {summary['vectordb']['status']}")

        if summary['vectordb']['status'] == 'success':
            logger.info(f"Total guides in VectorDB: {summary['vectordb']['total_guides']}")

        logger.info("=" * 80)

        # íŒŒì¼ë³„ ê²°ê³¼
        logger.info("\nğŸ“‹ FILE-BY-FILE RESULTS:")
        for file_result in summary['files']:
            status_emoji = "âœ…" if file_result['status'] == 'success' else "âŒ"
            logger.info(
                f"{status_emoji} {file_result['file']}: {file_result['status']} "
                f"({file_result.get('guides_count', 0)} guides)"
            )


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        processor = RawPDFBatchProcessor()

        # ëª¨ë“  PDF ì²˜ë¦¬
        results = await processor.process_all_pdfs()

        # VectorDB ë¹Œë“œ
        vectordb_result = processor.build_vectordb()

        # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
        summary = processor.save_results(results, vectordb_result)
        processor.print_summary(summary)

        logger.info("\nğŸ‰ Batch processing completed!")

    except KeyboardInterrupt:
        logger.info("\nâš ï¸  Processing interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
