import os
import json
import pickle
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from konlpy.tag import Okt
import torch
import numpy as np
from collections import defaultdict

class GuardcapRetriever:
    """
    Guardcap í”„ë¡œì íŠ¸ì˜ ê²€ìƒ‰ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° íŠ¹ìˆ˜í™”ëœ ê²€ìƒ‰ê¸°.
    - A(ì‚¬ë¡€): ë©”íƒ€ë°ì´í„° í•„í„°ë§ì´ ì ìš©ëœ BM25 ê²€ìƒ‰
    - B(ê·œì •), C(ë²•ë¥ ): ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰
    """

    @staticmethod
    def _nested_defaultdict_factory():
        return defaultdict(list)
    
    def __init__(self, index_base_path='./data/staging'):
        print("âœ… Guardcap ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   - ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")

        # 1. ì„ë² ë”© ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        self.model = SentenceTransformer('upskyy/e5-base-korean', device=device)
        self.tokenizer = Okt()
        print("   - ì„ë² ë”© ëª¨ë¸ ë° Okt í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")

        # 2. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        chroma_db_path = os.path.join(index_base_path, 'chroma_db')
        self.client = chromadb.PersistentClient(
            path=chroma_db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        print("   - ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
        
        # 3. A(ì‚¬ë¡€) ë°ì´í„° ë° BM25 ì¸ë±ìŠ¤ ë¡œë“œ
        self.bm25_model_A = None
        self.documents_A = []
        self.meta_index_A = {}
        self._load_a_cases_index(index_base_path)
        print("ğŸ‰ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì™„ë£Œ.")

    def _load_a_cases_index(self, index_base_path):
        """A_casesì˜ í†µí•© ì¸ë±ìŠ¤ íŒŒì¼(.pkl)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        index_path = os.path.join(index_base_path, 'bm25/A_cases_bm25.pkl')
        try:
            with open(index_path, 'rb') as f:
                data = pickle.load(f)
                self.bm25_model_A = data['bm25']
                self.documents_A = data['documents']
                self.meta_index_A = data['meta_index']
            print(f"   - A_cases í†µí•© ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({len(self.documents_A)}ê°œ ë¬¸ì„œ).")
        except FileNotFoundError:
            print(f"   - âš ï¸ ê²½ê³ : A_cases BM25 ì¸ë±ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {index_path}")
        except Exception as e:
            print(f"   - ğŸš¨ ì—ëŸ¬: A_cases ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # scripts/hybrid/retriever.py íŒŒì¼ ë‚´ë¶€

    def search_A_cases(self, query: str, filters: dict = None, top_k: int = 3) -> list:
        """
        A(ì‚¬ë¡€)ì— ëŒ€í•´ ë©”íƒ€ë°ì´í„° í•„í„°ë§ í›„ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if not self.bm25_model_A:
            print("ê²½ê³ : A_cases BM25 ì¸ë±ìŠ¤ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []

        # 1. í•„í„°ë§ì„ í†µí•´ ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ID ëª©ë¡(í›„ë³´êµ°) ì„ ì •
        candidate_indices = set(range(len(self.documents_A)))
        if filters:
            for field, value in filters.items():
                if field in self.meta_index_A and value in self.meta_index_A[field]:
                    candidate_indices.intersection_update(self.meta_index_A[field][value])
                else:
                    return []
        
        if not candidate_indices:
            return []

        # 2. BM25 ê²€ìƒ‰ (í›„ë³´êµ° ë‚´ì—ì„œë§Œ)
        tokenized_query = self.tokenizer.morphs(query)
        doc_scores = self.bm25_model_A.get_scores(tokenized_query)
        
        candidate_scores = {idx: doc_scores[idx] for idx in candidate_indices}

        # 3. ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Kê°œ ì •ë ¬
        sorted_indices = sorted(candidate_scores, key=candidate_scores.get, reverse=True)[:top_k]
        
        # 4. ê²°ê³¼ í¬ë§·íŒ…
        results = []
        for idx in sorted_indices:
            doc = self.documents_A[idx]
            results.append({
                "id": doc.get('case_id', 'N/A'),
                "score": candidate_scores[idx],
                "source": "A_cases",
                # âœ¨ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤.
                "snippet": (doc.get('before_text') or doc.get('after_text') or '')[:200],
                "meta": doc
            })
        return results

    def _search_vector_db(self, collection_name: str, query: str, top_k: int = 8) -> list:
        """ChromaDB ì»¬ë ‰ì…˜ì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ."""
        try:
            collection = self.client.get_collection(collection_name)
            query_embedding = self.model.encode(query).tolist()
            
            results_raw = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["metadatas", "documents", "distances"]
            )

            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            ids = results_raw['ids'][0]
            for i, doc_id in enumerate(ids):
                distance = results_raw['distances'][0][i]
                results.append({
                    "id": doc_id,
                    "score": 1 - distance,  # ì½”ì‚¬ì¸ ê±°ë¦¬(0~2)ë¥¼ ìœ ì‚¬ë„(0~1)ë¡œ ë³€í™˜
                    "source": collection_name,
                    "snippet": results_raw['documents'][0][i],
                    "meta": results_raw['metadatas'][0][i]
                })
            return results

        except Exception as e:
            # list_collections() ê²°ê³¼ê°€ ë¹„ì–´ìˆì„ ë•Œ ë°œìƒí•˜ëŠ” ValueError í¬í•¨
            print(f"ê²½ê³ : ChromaDB '{collection_name}' ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def search_B_policies(self, query: str, top_k: int = 8) -> list:
        """B(ê·œì •)ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        return self._search_vector_db('B_policies', query, top_k)

    def search_C_laws(self, query: str, top_k: int = 8) -> list:
        """C(ë²•ë¥ )ì— ëŒ€í•´ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        return self._search_vector_db('C_laws', query, top_k)


# --- ì‚¬ìš© ì˜ˆì‹œ ---
if __name__ == '__main__':
    retriever = GuardcapRetriever()
    print("\n--- ê²€ìƒ‰ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ---")

    # 1. A(ì‚¬ë¡€) ê²€ìƒ‰: í•„í„°ë§ ì ìš©
    print("\n[A] 'íŒŒíŠ¸ë„ˆ í¬í„¸'ì—ì„œ 'ì¢Œí‘œ' ìœ ì¶œ ì‚¬ë¡€ ê²€ìƒ‰ (í•„í„° ì ìš©)")
    query_a = "íŒŒíŠ¸ë„ˆ í¬í„¸ì—ì„œ í˜„ì¥ ì¢Œí‘œê°€ ìœ ì¶œë¨"
    filters_a = {'category': 'ì‚¬ì™¸', 'channel': 'WEB'}
    results_a = retriever.search_A_cases(query_a, filters=filters_a, top_k=3)
    if results_a:
        for res in results_a:
            print(f"  - ID: {res['id']}, Score: {res['score']:.4f}, Snippet: {res['snippet'][:50]}...")
    else:
        print("  - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")

    # 2. B(ê·œì •) ê²€ìƒ‰: ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰
    print("\n[B] 'ì£¼ì†Œ ê³µê°œ' ê´€ë ¨ ê·œì • ê²€ìƒ‰")
    query_b = "ì™¸ë¶€ íŒŒíŠ¸ë„ˆì—ê²Œ ì£¼ì†Œë¥¼ ê³µê°œí•´ë„ ë˜ë‚˜ìš”?"
    results_b = retriever.search_B_policies(query_b, top_k=5)
    if results_b:
        for res in results_b:
            print(f"  - ID: {res['id']}, Score: {res['score']:.4f}, Snippet: {res['snippet'][:50]}...")
    else:
        print("  - ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")