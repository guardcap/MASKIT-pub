"""
Raw Guidelines ë°°ì¹˜ ì²˜ë¦¬ í˜ì´ì§€
raw_guidelines ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDFë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ VectorDBì— ì¶”ê°€
"""

import streamlit as st
from pathlib import Path
import asyncio
import json
import sys
import os
from datetime import datetime
import tempfile
import logging

# litellm ë¡œê¹… ë¹„í™œì„±í™”
import litellm
litellm.disable_logging = True

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.guidelines.process_guidelines import GuidelineProcessor
from scripts.guidelines.build_guides_vectordb import GuidesVectorDBBuilder
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(
    page_title="Batch Process - Guardcap RAG",
    page_icon="âš™ï¸",
    layout="wide"
)

st.title("âš™ï¸ Batch Process Raw Guidelines")
st.markdown("---")
st.markdown("`raw_guidelines` ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  PDFë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ VectorDBì— ì¶”ê°€í•©ë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class AuthorityDetector:
    """íŒŒì¼ëª…ì—ì„œ ë°œí–‰ ê¸°ê´€ ê°ì§€"""

    AUTHORITY_KEYWORDS = {
        "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": ["ê°œì¸ì •ë³´", "ë³´í˜¸"],
        "ê¸ˆìœµë³´ì•ˆì›": ["ê¸ˆìœµ", "ë³´ì•ˆ"],
        "KISA": ["KISA", "ì •ë³´ë³´í˜¸"],
        "ê³µì •ê±°ë˜ìœ„ì›íšŒ": ["ê³µì •ê±°ë˜"],
        "ê¸ˆìœµìœ„ì›íšŒ": ["ê¸ˆìœµìœ„"],
        "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€": ["ê³¼í•™ê¸°ìˆ ", "í†µì‹ "],
        "í–‰ì •ì•ˆì „ë¶€": ["í–‰ì •ì•ˆì „", "ì •ë³´ë³´í˜¸"],
    }

    @staticmethod
    def detect(filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ë°œí–‰ ê¸°ê´€ ê°ì§€"""
        filename_lower = filename.lower()

        # ì •í™•í•œ ë§¤ì¹­
        for authority, keywords in AuthorityDetector.AUTHORITY_KEYWORDS.items():
            if all(kw.lower() in filename_lower for kw in keywords):
                return authority

        # ë¶€ë¶„ ë§¤ì¹­
        for authority, keywords in AuthorityDetector.AUTHORITY_KEYWORDS.items():
            if any(kw.lower() in filename_lower for kw in keywords):
                return authority

        # ê¸°ë³¸ê°’
        return "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ"


# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ Processing Settings")

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        st.success("âœ… OpenAI API Key ì„¤ì •ë¨")
    else:
        st.error("âŒ OPENAI_API_KEY í•„ìš”")
        st.info("`.env` íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")

    st.markdown("---")

    # ì²˜ë¦¬ ì˜µì…˜
    st.subheader("Processing Options")
    vision_model = st.selectbox(
        "Vision Model",
        ["gpt-4o-mini", "gpt-4o"],
        help="gpt-4o-mini: ë¹ ë¦„ (2-3ë°°), gpt-4o: ì •í™•í•¨"
    )

    batch_delay = st.slider(
        "ë°°ì¹˜ ê°„ ë”œë ˆì´ (ì´ˆ)",
        min_value=1,
        max_value=10,
        value=3,
        help="Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ PDF ê°„ ëŒ€ê¸° ì‹œê°„"
    )

# ë©”ì¸ ì»¨í…ì¸ 
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ“‚ Raw Guidelines Directory")

    raw_dir = Path("data/raw_guidelines")

    if not raw_dir.exists():
        st.error(f"âŒ Directory not found: {raw_dir}")
    else:
        # PDF íŒŒì¼ ëª©ë¡
        pdf_files = list(raw_dir.glob("*.pdf"))
        pdf_files.sort()

        st.info(f"ğŸ“ Found **{len(pdf_files)}** PDF files")

        if pdf_files:
            # íŒŒì¼ ëª©ë¡ í‘œì‹œ
            with st.expander(f"ğŸ“‹ File List ({len(pdf_files)} files)", expanded=False):
                file_data = []
                for pdf_file in pdf_files:
                    file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
                    authority = AuthorityDetector.detect(pdf_file.name)
                    file_data.append({
                        "íŒŒì¼ëª…": pdf_file.name[:60] + ("..." if len(pdf_file.name) > 60 else ""),
                        "í¬ê¸° (MB)": f"{file_size_mb:.2f}",
                        "ê¸°ê´€": authority
                    })

                st.dataframe(file_data, use_container_width=True, hide_index=True)

with col2:
    st.subheader("ğŸ“Š ì²˜ë¦¬ ì •ë³´")

    if pdf_files:
        total_size = sum(p.stat().st_size for p in pdf_files) / (1024 * 1024)
        avg_size = total_size / len(pdf_files)

        st.metric("ì´ íŒŒì¼ ìˆ˜", len(pdf_files))
        st.metric("ì´ í¬ê¸°", f"{total_size:.1f} MB")
        st.metric("í‰ê·  íŒŒì¼ í¬ê¸°", f"{avg_size:.1f} MB")

        # ì²˜ë¦¬ ì‹œê°„ ì¶”ì •
        est_time_min = (len(pdf_files) * 45) / 60  # ëŒ€ëµ PDFë‹¹ 45ì´ˆ
        est_time_max = (len(pdf_files) * 90) / 60
        st.metric("ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„", f"{est_time_min:.0f}-{est_time_max:.0f}ë¶„")
    else:
        st.warning("ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

st.markdown("---")

# ì²˜ë¦¬ ë²„íŠ¼
if api_key and pdf_files:
    if st.button("ğŸš€ Start Batch Processing", type="primary", use_container_width=True):
        st.session_state.processing = True

    if "processing" in st.session_state and st.session_state.processing:
        try:
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_container = st.container()

            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            os.environ["PDF_BATCH_DELAY"] = str(batch_delay)
            os.environ["OPENAI_VISION_MODEL"] = vision_model

            # ê²°ê³¼ ì €ì¥ì†Œ
            results = []
            all_guides_extracted = 0

            # 1. ëª¨ë“  PDF ì²˜ë¦¬
            with status_container:
                st.info("ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")

            for file_idx, pdf_file in enumerate(pdf_files):
                file_size_mb = pdf_file.stat().st_size / (1024 * 1024)
                progress = (file_idx / len(pdf_files)) * 90

                with status_container:
                    st.write(
                        f"ğŸ“„ [{file_idx + 1}/{len(pdf_files)}] "
                        f"{pdf_file.name[:50]}... ({file_size_mb:.2f}MB)"
                    )

                try:
                    # ë°œí–‰ ê¸°ê´€ ê°ì§€
                    authority = AuthorityDetector.detect(pdf_file.name)

                    # PDF ì²˜ë¦¬
                    processor = GuidelineProcessor(
                        openai_api_key=api_key,
                        vision_model=vision_model,
                        output_dir="data/staging"
                    )

                    async def process_pdf():
                        return await processor.process_pdf(str(pdf_file), authority)

                    guides = asyncio.run(process_pdf())

                    if guides:
                        # JSONL ì €ì¥
                        async def save_guides():
                            return await processor.save_guides(
                                guides,
                                suffix=f"batch_{file_idx:02d}"
                            )

                        output_file = asyncio.run(save_guides())

                        results.append({
                            "íŒŒì¼": pdf_file.name,
                            "ìƒíƒœ": "âœ… ì„±ê³µ",
                            "ê°€ì´ë“œ ìˆ˜": len(guides),
                            "íŒŒì¼ í¬ê¸°": f"{file_size_mb:.2f}MB",
                            "ê¸°ê´€": authority
                        })

                        all_guides_extracted += len(guides)

                        with status_container:
                            st.success(
                                f"âœ… {pdf_file.name}: {len(guides)}ê°œ ê°€ì´ë“œ ì¶”ì¶œ"
                            )
                    else:
                        results.append({
                            "íŒŒì¼": pdf_file.name,
                            "ìƒíƒœ": "âš ï¸ ë°ì´í„° ì—†ìŒ",
                            "ê°€ì´ë“œ ìˆ˜": 0,
                            "íŒŒì¼ í¬ê¸°": f"{file_size_mb:.2f}MB",
                            "ê¸°ê´€": authority
                        })

                        with status_container:
                            st.warning(f"âš ï¸ {pdf_file.name}: ê°€ì´ë“œ ì¶”ì¶œ ì‹¤íŒ¨")

                    # Rate Limit ë”œë ˆì´ (ì²« íŒŒì¼ ì œì™¸)
                    if file_idx < len(pdf_files) - 1:
                        with status_container:
                            st.info(f"â¸ï¸ {batch_delay}ì´ˆ ëŒ€ê¸° ì¤‘... (Rate Limit ë°©ì§€)")
                        asyncio.run(asyncio.sleep(batch_delay))

                    progress_bar.progress(min(progress + 10, 90))

                except Exception as e:
                    results.append({
                        "íŒŒì¼": pdf_file.name,
                        "ìƒíƒœ": "âŒ ì˜¤ë¥˜",
                        "ê°€ì´ë“œ ìˆ˜": 0,
                        "íŒŒì¼ í¬ê¸°": f"{file_size_mb:.2f}MB",
                        "ê¸°ê´€": "ì˜¤ë¥˜ ë°œìƒ"
                    })

                    with status_container:
                        st.error(f"âŒ {pdf_file.name}: {str(e)[:100]}")

            # 2. VectorDB ë¹Œë“œ
            with status_container:
                st.info("ğŸ” VectorDB ë¹Œë“œ ì¤‘...")

            progress_bar.progress(91)

            builder = GuidesVectorDBBuilder(
                openai_api_key=api_key,
                db_path="data/chromadb/application_guides"
            )

            staging_dir = Path("data/staging")
            all_guides = builder.load_all_guides_from_directory(
                str(staging_dir),
                pattern="application_guides_*.jsonl"
            )

            if all_guides:
                builder.add_guides_to_db(all_guides, batch_size=50)

            progress_bar.progress(100)

            # 3. ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.success("### âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")

            # ê²°ê³¼ ìš”ì•½
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("ì²˜ë¦¬ëœ íŒŒì¼", len(results))
            col2.metric("ì¶”ì¶œëœ ê°€ì´ë“œ", all_guides_extracted)
            col3.metric("VectorDB ì´ê³„", len(all_guides) if all_guides else 0)
            col4.metric("ì„±ê³µë¥ ", f"{sum(1 for r in results if 'âœ…' in r['ìƒíƒœ']) / len(results) * 100:.0f}%")

            # íŒŒì¼ë³„ ê²°ê³¼
            st.subheader("ğŸ“‹ File Processing Results")
            st.dataframe(results, use_container_width=True, hide_index=True)

            # ì €ì¥ëœ JSONL íŒŒì¼ ëª©ë¡
            st.subheader("ğŸ“ Saved JSONL Files")
            jsonl_files = list(staging_dir.glob("application_guides_*batch*.jsonl"))
            if jsonl_files:
                jsonl_data = []
                for jsonl_file in sorted(jsonl_files, reverse=True):
                    file_size_kb = jsonl_file.stat().st_size / 1024
                    guide_count = sum(1 for line in open(jsonl_file, encoding="utf-8") if line.strip())
                    jsonl_data.append({
                        "íŒŒì¼ëª…": jsonl_file.name,
                        "ê°€ì´ë“œ ìˆ˜": guide_count,
                        "í¬ê¸° (KB)": f"{file_size_kb:.1f}"
                    })

                st.dataframe(jsonl_data, use_container_width=True, hide_index=True)

            st.session_state.processing = False

        except Exception as e:
            st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)
            st.session_state.processing = False

else:
    if not api_key:
        st.warning("âŒ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    if not pdf_files:
        st.warning("âŒ raw_guidelines ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.info("""
### ğŸ“ ì‚¬ìš© ë°©ë²•

1. **API Key ì„¤ì •**: `.env` íŒŒì¼ì— `OPENAI_API_KEY` ì„¤ì •
2. **ì‚¬ì´ë“œë°” ì˜µì…˜**: Vision ëª¨ë¸ê³¼ ë”œë ˆì´ ì¡°ì •
3. **ë²„íŠ¼ í´ë¦­**: "ğŸš€ Start Batch Processing" ë²„íŠ¼ í´ë¦­
4. **ì§„í–‰ ìƒí™© í™•ì¸**: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
5. **ê²€ìƒ‰ í…ŒìŠ¤íŠ¸**: "ğŸ” Search Guidelines" í˜ì´ì§€ì—ì„œ ê²€ìƒ‰

### âš ï¸ ì£¼ì˜ì‚¬í•­

- ì²˜ë¦¬ ì¤‘ í˜ì´ì§€ë¥¼ ë‚˜ê°€ì§€ ë§ˆì„¸ìš”
- Rate Limit ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë”œë ˆì´ë¥¼ ì¦ê°€ì‹œì¼œë³´ì„¸ìš”
- í° íŒŒì¼ì€ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì²˜ë¦¬ ì¤‘ë‹¨ì€ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ì´ì–´ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤
""")
