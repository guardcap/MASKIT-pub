"""
PDF ì—…ë¡œë“œ ë° VectorDB ì¶”ê°€ í˜ì´ì§€
"""

import streamlit as st
from pathlib import Path
import asyncio
import json
import sys
import os
from datetime import datetime
import tempfile
import litellm

# litellmì˜ ë°±ê·¸ë¼ìš´ë“œ ë¡œê¹… ì›Œì»¤ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
litellm.disable_logging = True

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.guidelines.process_guidelines import GuidelineProcessor
from scripts.guidelines.build_guides_vectordb import GuidesVectorDBBuilder
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(
    page_title="Upload PDF - Guardcap RAG",
    page_icon="ğŸ“„",
    layout="wide"
)

st.title("ğŸ“„ Upload & Process Guidelines PDF")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("âš™ï¸ Processing Settings")

    authority = st.selectbox(
        "ë°œí–‰ ê¸°ê´€",
        [
            "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ",
            "ê¸ˆìœµë³´ì•ˆì›",
            "ê¸ˆìœµìœ„ì›íšŒ",
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ",
            "KISA (í•œêµ­ì¸í„°ë„·ì§„í¥ì›)",
            "ê¸°íƒ€"
        ]
    )

    if authority == "ê¸°íƒ€":
        authority = st.text_input("ê¸°ê´€ëª… ì…ë ¥", "")

    st.markdown("---")

    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        st.success("âœ… OpenAI API Key ì„¤ì •ë¨")
    else:
        st.error("âŒ OPENAI_API_KEY í•„ìš”")
        st.info("`.env` íŒŒì¼ì— API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”")

    st.markdown("---")

    # ì²˜ë¦¬ ì˜µì…˜
    st.subheader("Processing Options")
    vision_model = st.selectbox(
        "Vision Model",
        ["gpt-4o-mini", "gpt-4o"],
        help="gpt-4o-mini: ë¹ ë¦„ (2-3ë°°), gpt-4o: ì •í™•í•¨"
    )

    batch_size = st.slider(
        "ë°°ì¹˜ í¬ê¸° (í˜ì´ì§€)",
        min_value=5,
        max_value=30,
        value=20,
        help="ëŒ€ìš©ëŸ‰ PDF ë¶„í•  ì‹œ ë°°ì¹˜ ë‹¹ í˜ì´ì§€ ìˆ˜"
    )

# ë©”ì¸ ì»¨í…ì¸ 
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ“¤ Upload PDF File")

    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ ì„ íƒ",
        type=["pdf"],
        help="ì‹¤ë¬´ ê°€ì´ë“œë¼ì¸ PDFë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ìµœëŒ€ 100MB)"
    )

    if uploaded_file:
        file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
        st.info(f"ğŸ“ **File**: {uploaded_file.name} ({file_size_mb:.2f} MB)")

        if file_size_mb > 100:
            st.error("âŒ íŒŒì¼ í¬ê¸°ê°€ 100MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
        else:
            st.success("âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")

with col2:
    st.subheader("ğŸ“Š ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„")

    if uploaded_file:
        # í˜ì´ì§€ ìˆ˜ ì¶”ì • (ëŒ€ëµ 1MB = 15-20í˜ì´ì§€)
        est_pages = int(file_size_mb * 17)

        if vision_model == "gpt-4o-mini":
            est_time_min = est_pages * 0.05  # ~3ì´ˆ/í˜ì´ì§€
            est_time_max = est_pages * 0.08
        else:
            est_time_min = est_pages * 0.1   # ~6ì´ˆ/í˜ì´ì§€
            est_time_max = est_pages * 0.15

        st.metric("ì˜ˆìƒ í˜ì´ì§€", f"~{est_pages}p")
        st.metric("ì˜ˆìƒ ì‹œê°„", f"{est_time_min:.1f}-{est_time_max:.1f}ë¶„")
        st.metric("ë°°ì¹˜ ìˆ˜", f"~{est_pages // batch_size + 1}")
    else:
        st.info("PDFë¥¼ ì—…ë¡œë“œí•˜ë©´ ì˜ˆìƒ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤")

st.markdown("---")

# ì²˜ë¦¬ ë²„íŠ¼
if uploaded_file and api_key and authority:
    if st.button("ğŸš€ Process PDF", type="primary", use_container_width=True):
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        try:
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0)
            status_text = st.empty()

            # Step 1: PDF ì²˜ë¦¬
            status_text.text("ğŸ“„ Step 1/3: Processing PDF with Vision OCR...")
            status_text.text(f"âš™ï¸  Settings: {vision_model}, ë°°ì¹˜={batch_size}í˜ì´ì§€")
            progress_bar.progress(10)

            # ë°°ì¹˜ í¬ê¸°ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • (GuidelineProcessor ìƒì„± ì „ì—!)
            os.environ["PDF_BATCH_SIZE"] = str(batch_size)

            processor = GuidelineProcessor(
                openai_api_key=api_key,
                vision_model=vision_model
            )

            # ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤í–‰
            async def process():
                return await processor.process_pdf(tmp_path, authority)

            guides = asyncio.run(process())
            progress_bar.progress(50)

            if not guides:
                st.error("âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: ê°€ì´ë“œë¼ì¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                status_text.text(f"âœ… Step 1 ì™„ë£Œ: {len(guides)}ê°œ ê°€ì´ë“œë¼ì¸ ì¶”ì¶œë¨")

                # Step 2: JSONL ì €ì¥ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…)
                status_text.text("ğŸ’¾ Step 2/3: Saving to JSONL...")
                progress_bar.progress(60)

                # save_guides ë©”ì„œë“œ ì‚¬ìš©
                async def save():
                    return await processor.save_guides(guides, suffix="uploaded")

                output_file = asyncio.run(save())

                progress_bar.progress(70)
                status_text.text(f"âœ… Step 2 ì™„ë£Œ: {Path(output_file).name}")

                # Step 3: VectorDBì— ì¶”ê°€
                status_text.text("ğŸ” Step 3/3: Adding to VectorDB...")
                progress_bar.progress(80)

                builder = GuidesVectorDBBuilder(
                    openai_api_key=api_key,
                    db_path="data/chromadb/application_guides"
                )

                # staging í´ë”ì˜ ëª¨ë“  unique/uploaded íŒŒì¼ ë¡œë“œ
                staging_dir = Path("data/staging")
                all_guides = builder.load_all_guides_from_directory(
                    str(staging_dir),
                    pattern="application_guides_*_{uploaded,unique}.jsonl"
                )

                if not all_guides:
                    # Fallback: ëª¨ë“  íŒŒì¼ (review ì œì™¸)
                    all_guides = []
                    for jsonl_file in staging_dir.glob("application_guides_*.jsonl"):
                        if "review" not in jsonl_file.name.lower():
                            with open(jsonl_file, "r", encoding="utf-8") as f:
                                for line in f:
                                    if line.strip():
                                        all_guides.append(json.loads(line))

                # VectorDB ì¬ë¹Œë“œ
                builder.add_guides_to_db(all_guides, batch_size=50)

                progress_bar.progress(100)
                status_text.text("âœ… All steps completed!")

                # ê²°ê³¼ í‘œì‹œ
                st.success(f"""
                ### âœ… ì²˜ë¦¬ ì™„ë£Œ!

                - **ì¶”ì¶œëœ ê°€ì´ë“œë¼ì¸**: {len(guides)}ê°œ
                - **ì €ì¥ ìœ„ì¹˜**: `{Path(output_file).name}`
                - **VectorDB**: ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(all_guides)}ê°œ)
                """)

                # ìƒ˜í”Œ í‘œì‹œ
                st.markdown("---")
                st.subheader("ğŸ“‹ ì¶”ì¶œëœ ê°€ì´ë“œë¼ì¸ ìƒ˜í”Œ")

                for i, guide in enumerate(guides[:3]):
                    with st.expander(f"Guide {i+1}: {guide.scenario[:100]}..."):
                        st.json(guide.dict())

                # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
                st.info("""
                ### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

                1. **ğŸ” Search Guidelines** í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                2. í•„ìš”ì‹œ `review_queue.csv` í™•ì¸í•˜ì—¬ ë‚®ì€ ì‹ ë¢°ë„ í•­ëª© ê²€í† 
                3. ì¤‘ë³µ ì œê±°: `python scripts/guidelines/validate_and_dedup.py` ì‹¤í–‰
                """)

        except Exception as e:
            st.error(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.unlink(tmp_path)
            except:
                pass

else:
    st.warning("""
    ### âš ï¸ ì²˜ë¦¬ ì „ í™•ì¸ ì‚¬í•­

    - âœ… PDF íŒŒì¼ ì—…ë¡œë“œ
    - âœ… ë°œí–‰ ê¸°ê´€ ì„ íƒ
    - âœ… OpenAI API Key ì„¤ì • (`.env` íŒŒì¼)
    """)

# ê¸°ì¡´ íŒŒì¼ ëª©ë¡
st.markdown("---")
st.subheader("ğŸ“‚ Existing Guidelines Files")

staging_dir = Path("data/staging")
if staging_dir.exists():
    jsonl_files = [f for f in staging_dir.glob("application_guides_*.jsonl")
                   if "review" not in f.name.lower() and "duplicates" not in f.name.lower()]

    if jsonl_files:
        st.info(f"ì´ {len(jsonl_files)}ê°œ íŒŒì¼ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…)")

        for jsonl_file in sorted(jsonl_files, reverse=True)[:10]:  # ìµœê·¼ 10ê°œë§Œ
            file_size = jsonl_file.stat().st_size / 1024  # KB
            line_count = sum(1 for line in open(jsonl_file, encoding="utf-8") if line.strip())

            col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
            col1.text(f"ğŸ“„ {jsonl_file.name}")
            col2.text(f"{line_count} guides")
            col3.text(f"{file_size:.1f} KB")

            # ë¯¸ë¦¬ë³´ê¸° ë²„íŠ¼
            if col4.button("Preview", key=f"preview_{jsonl_file.name}"):
                with st.expander(f"Preview: {jsonl_file.name}", expanded=True):
                    with open(jsonl_file, "r", encoding="utf-8") as f:
                        for i, line in enumerate(f):
                            if i >= 3:
                                break
                            if line.strip():
                                st.json(json.loads(line))
    else:
        st.info("ì•„ì§ ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("staging ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
